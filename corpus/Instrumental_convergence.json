{
  "url": "https://en.wikipedia.org/wiki/Instrumental_convergence",
  "title": "Instrumental convergence",
  "html": "<!DOCTYPE html>\n<html class=\"client-nojs\" dir=\"ltr\" lang=\"en\">\n<head>\n<meta charset=\"utf-8\"/>\n<title>Instrumental convergence - Wikipedia</title>\n<script>document.documentElement.className = document.documentElement.className.replace( /(^|\\s)client-nojs(\\s|$)/, \"$1client-js$2\" );</script>\n<script>(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({\"wgCanonicalNamespace\":\"\",\"wgCanonicalSpecialPageName\":false,\"wgNamespaceNumber\":0,\"wgPageName\":\"Instrumental_convergence\",\"wgTitle\":\"Instrumental convergence\",\"wgCurRevisionId\":884577623,\"wgRevisionId\":884577623,\"wgArticleId\":43591208,\"wgIsArticle\":true,\"wgIsRedirect\":false,\"wgAction\":\"view\",\"wgUserName\":null,\"wgUserGroups\":[\"*\"],\"wgCategories\":[\"Articles with incomplete citations from September 2018\",\"All articles with incomplete citations\",\"Wikipedia articles needing page number citations from September 2018\",\"Artificial intelligence\",\"Goal\",\"Intention\",\"Risk\",\"Artificial Intelligence existential risk\"],\"wgBreakFrames\":false,\"wgPageContentLanguage\":\"en\",\"wgPageContentModel\":\"wikitext\",\"wgSeparatorTransformTable\":[\"\",\"\"],\"wgDigitTransformTable\":[\"\",\"\"],\"wgDefaultDateFormat\":\"dmy\",\"wgMonthNames\":[\"\",\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"December\"],\"wgMonthNamesShort\":[\"\",\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"],\"wgRelevantPageName\":\"Instrumental_convergence\",\"wgRelevantArticleId\":43591208,\"wgRequestId\":\"XHGjaApAMEwAAKDKDUQAAAAO\",\"wgCSPNonce\":false,\"wgIsProbablyEditable\":true,\"wgRelevantPageIsProbablyEditable\":true,\"wgRestrictionEdit\":[],\"wgRestrictionMove\":[],\"wgFlaggedRevsParams\":{\"tags\":{}},\"wgStableRevisionId\":null,\"wgCategoryTreePageCategoryOptions\":\"{\\\"mode\\\":0,\\\"hideprefix\\\":20,\\\"showcount\\\":true,\\\"namespaces\\\":false}\",\"wgWikiEditorEnabledModules\":[],\"wgBetaFeaturesFeatures\":[],\"wgMediaViewerOnClick\":true,\"wgMediaViewerEnabledByDefault\":true,\"wgPopupsShouldSendModuleToUser\":true,\"wgPopupsConflictsWithNavPopupGadget\":false,\"wgVisualEditor\":{\"pageLanguageCode\":\"en\",\"pageLanguageDir\":\"ltr\",\"pageVariantFallbacks\":\"en\",\"usePageImages\":true,\"usePageDescriptions\":true},\"wgMFIsPageContentModelEditable\":true,\"wgMFEnableFontChanger\":true,\"wgMFDisplayWikibaseDescriptions\":{\"search\":true,\"nearby\":true,\"watchlist\":true,\"tagline\":false},\"wgRelatedArticles\":null,\"wgRelatedArticlesUseCirrusSearch\":true,\"wgRelatedArticlesOnlyUseCirrusSearch\":false,\"wgWMESchemaEditAttemptStepOversample\":false,\"wgPoweredByHHVM\":true,\"wgULSCurrentAutonym\":\"English\",\"wgNoticeProject\":\"wikipedia\",\"wgCentralNoticeCookiesToDelete\":[],\"wgCentralNoticeCategoriesUsingLegacy\":[\"Fundraising\",\"fundraising\"],\"wgWikibaseItemId\":\"Q18208100\",\"wgScoreNoteLanguages\":{\"arabic\":\"العربية\",\"catalan\":\"català\",\"deutsch\":\"Deutsch\",\"english\":\"English\",\"espanol\":\"español\",\"italiano\":\"italiano\",\"nederlands\":\"Nederlands\",\"norsk\":\"norsk\",\"portugues\":\"português\",\"suomi\":\"suomi\",\"svenska\":\"svenska\",\"vlaams\":\"West-Vlams\"},\"wgScoreDefaultNoteLanguage\":\"nederlands\",\"wgCentralAuthMobileDomain\":false,\"wgCodeMirrorEnabled\":true,\"wgVisualEditorToolbarScrollOffset\":0,\"wgVisualEditorUnsupportedEditParams\":[\"undo\",\"undoafter\",\"veswitched\"],\"wgEditSubmitButtonLabelPublish\":true,\"oresWikiId\":\"enwiki\",\"oresBaseUrl\":\"http://ores.discovery.wmnet:8081/\",\"oresApiVersion\":3});mw.loader.state({\"ext.gadget.charinsert-styles\":\"ready\",\"ext.globalCssJs.user.styles\":\"ready\",\"ext.globalCssJs.site.styles\":\"ready\",\"site.styles\":\"ready\",\"noscript\":\"ready\",\"user.styles\":\"ready\",\"ext.globalCssJs.user\":\"ready\",\"ext.globalCssJs.site\":\"ready\",\"user\":\"ready\",\"user.options\":\"ready\",\"user.tokens\":\"loading\",\"ext.cite.styles\":\"ready\",\"mediawiki.legacy.shared\":\"ready\",\"mediawiki.legacy.commonPrint\":\"ready\",\"mediawiki.toc.styles\":\"ready\",\"wikibase.client.init\":\"ready\",\"ext.visualEditor.desktopArticleTarget.noscript\":\"ready\",\"ext.uls.interlanguage\":\"ready\",\"ext.wikimediaBadges\":\"ready\",\"ext.3d.styles\":\"ready\",\"mediawiki.skinning.interface\":\"ready\",\"skins.vector.styles\":\"ready\"});mw.loader.implement(\"user.tokens@0tffind\",function($,jQuery,require,module){/*@nomin*/mw.user.tokens.set({\"editToken\":\"+\\\\\",\"patrolToken\":\"+\\\\\",\"watchToken\":\"+\\\\\",\"csrfToken\":\"+\\\\\"});\n});RLPAGEMODULES=[\"ext.cite.ux-enhancements\",\"site\",\"mediawiki.page.startup\",\"mediawiki.page.ready\",\"mediawiki.toc\",\"mediawiki.searchSuggest\",\"ext.gadget.teahouse\",\"ext.gadget.ReferenceTooltips\",\"ext.gadget.watchlist-notice\",\"ext.gadget.DRN-wizard\",\"ext.gadget.charinsert\",\"ext.gadget.refToolbar\",\"ext.gadget.extra-toolbar-buttons\",\"ext.gadget.switcher\",\"ext.centralauth.centralautologin\",\"ext.popups\",\"ext.visualEditor.desktopArticleTarget.init\",\"ext.visualEditor.targetLoader\",\"ext.eventLogging\",\"ext.wikimediaEvents\",\"ext.navigationTiming\",\"ext.uls.eventlogger\",\"ext.uls.init\",\"ext.uls.compactlinks\",\"ext.uls.interface\",\"ext.quicksurveys.init\",\"ext.centralNotice.geoIP\",\"ext.centralNotice.startUp\",\"skins.vector.js\"];mw.loader.load(RLPAGEMODULES);});</script>\n<link href=\"/w/load.php?debug=false&amp;lang=en&amp;modules=ext.3d.styles%7Cext.cite.styles%7Cext.uls.interlanguage%7Cext.visualEditor.desktopArticleTarget.noscript%7Cext.wikimediaBadges%7Cmediawiki.legacy.commonPrint%2Cshared%7Cmediawiki.skinning.interface%7Cmediawiki.toc.styles%7Cskins.vector.styles%7Cwikibase.client.init&amp;only=styles&amp;skin=vector\" rel=\"stylesheet\"/>\n<script async=\"\" src=\"/w/load.php?debug=false&amp;lang=en&amp;modules=startup&amp;only=scripts&amp;skin=vector\"></script>\n<meta content=\"\" name=\"ResourceLoaderDynamicStyles\"/>\n<link href=\"/w/load.php?debug=false&amp;lang=en&amp;modules=ext.gadget.charinsert-styles&amp;only=styles&amp;skin=vector\" rel=\"stylesheet\"/>\n<link href=\"/w/load.php?debug=false&amp;lang=en&amp;modules=site.styles&amp;only=styles&amp;skin=vector\" rel=\"stylesheet\"/>\n<meta content=\"MediaWiki 1.33.0-wmf.18\" name=\"generator\"/>\n<meta content=\"origin\" name=\"referrer\"/>\n<meta content=\"origin-when-crossorigin\" name=\"referrer\"/>\n<meta content=\"origin-when-cross-origin\" name=\"referrer\"/>\n<link href=\"android-app://org.wikipedia/http/en.m.wikipedia.org/wiki/Instrumental_convergence\" rel=\"alternate\"/>\n<link href=\"/w/index.php?title=Instrumental_convergence&amp;action=edit\" rel=\"alternate\" title=\"Edit this page\" type=\"application/x-wiki\"/>\n<link href=\"/w/index.php?title=Instrumental_convergence&amp;action=edit\" rel=\"edit\" title=\"Edit this page\"/>\n<link href=\"/static/apple-touch/wikipedia.png\" rel=\"apple-touch-icon\"/>\n<link href=\"/static/favicon/wikipedia.ico\" rel=\"shortcut icon\"/>\n<link href=\"/w/opensearch_desc.php\" rel=\"search\" title=\"Wikipedia (en)\" type=\"application/opensearchdescription+xml\"/>\n<link href=\"//en.wikipedia.org/w/api.php?action=rsd\" rel=\"EditURI\" type=\"application/rsd+xml\"/>\n<link href=\"//creativecommons.org/licenses/by-sa/3.0/\" rel=\"license\"/>\n<link href=\"https://en.wikipedia.org/wiki/Instrumental_convergence\" rel=\"canonical\"/>\n<link href=\"//login.wikimedia.org\" rel=\"dns-prefetch\"/>\n<link href=\"//meta.wikimedia.org\" rel=\"dns-prefetch\"/>\n<!--[if lt IE 9]><script src=\"/w/load.php?debug=false&amp;lang=en&amp;modules=html5shiv&amp;only=scripts&amp;skin=vector&amp;sync=1\"></script><![endif]-->\n</head>\n<body class=\"mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject mw-editable page-Instrumental_convergence rootpage-Instrumental_convergence skin-vector action-view\"> <div class=\"noprint\" id=\"mw-page-base\"></div>\n<div class=\"noprint\" id=\"mw-head-base\"></div>\n<div class=\"mw-body\" id=\"content\" role=\"main\">\n<a id=\"top\"></a>\n<div class=\"mw-body-content\" id=\"siteNotice\"><!-- CentralNotice --></div><div class=\"mw-indicators mw-body-content\">\n</div>\n<h1 class=\"firstHeading\" id=\"firstHeading\" lang=\"en\">Instrumental convergence</h1> <div class=\"mw-body-content\" id=\"bodyContent\">\n<div class=\"noprint\" id=\"siteSub\">From Wikipedia, the free encyclopedia</div> <div id=\"contentSub\"></div>\n<div id=\"jump-to-nav\"></div> <a class=\"mw-jump-link\" href=\"#mw-head\">Jump to navigation</a>\n<a class=\"mw-jump-link\" href=\"#p-search\">Jump to search</a>\n<div class=\"mw-content-ltr\" dir=\"ltr\" id=\"mw-content-text\" lang=\"en\"><div class=\"mw-parser-output\"><p><b>Instrumental convergence</b> is the hypothetical tendency for most sufficiently <a href=\"/wiki/Intelligent_agent\" title=\"Intelligent agent\">intelligent agents</a> to pursue potentially unbounded instrumental goals such as <a href=\"/wiki/Self-preservation\" title=\"Self-preservation\">self-preservation</a> and <a href=\"/wiki/Resource\" title=\"Resource\">resource</a> acquisition, provided that their ultimate goals are themselves unbounded.\n</p><p>Instrumental convergence suggests that an intelligent agent with unbounded but apparently harmless goals can act in surprisingly harmful ways. For example, a computer with the sole, unconstrained goal of solving the <a href=\"/wiki/Riemann_hypothesis\" title=\"Riemann hypothesis\">Riemann hypothesis</a> could attempt to turn the entire Earth into <a href=\"/wiki/Computronium\" title=\"Computronium\">computronium</a> in an effort to increase its computing power so that it can succeed in its calculations.<sup class=\"reference\" id=\"cite_ref-aama_1-0\"><a href=\"#cite_note-aama-1\">[1]</a></sup>\n</p><p>Proposed <b>basic AI drives</b> include utility function or goal-content integrity, self-protection, freedom from interference, <a class=\"mw-redirect\" href=\"/wiki/Self-improvement\" title=\"Self-improvement\">self-improvement</a>, and non-satiable acquisition of additional resources.\n</p>\n<div class=\"toc\" id=\"toc\"><input class=\"toctogglecheckbox\" id=\"toctogglecheckbox\" role=\"button\" style=\"display:none\" type=\"checkbox\"/><div class=\"toctitle\" dir=\"ltr\" lang=\"en\"><h2>Contents</h2><span class=\"toctogglespan\"><label class=\"toctogglelabel\" for=\"toctogglecheckbox\"></label></span></div>\n<ul>\n<li class=\"toclevel-1 tocsection-1\"><a href=\"#Instrumental_and_final_goals\"><span class=\"tocnumber\">1</span> <span class=\"toctext\">Instrumental and final goals</span></a></li>\n<li class=\"toclevel-1 tocsection-2\"><a href=\"#Hypothetical_examples_of_convergence\"><span class=\"tocnumber\">2</span> <span class=\"toctext\">Hypothetical examples of convergence</span></a>\n<ul>\n<li class=\"toclevel-2 tocsection-3\"><a href=\"#Paperclip_maximizer\"><span class=\"tocnumber\">2.1</span> <span class=\"toctext\">Paperclip maximizer</span></a></li>\n</ul>\n</li>\n<li class=\"toclevel-1 tocsection-4\"><a href=\"#Basic_AI_drives\"><span class=\"tocnumber\">3</span> <span class=\"toctext\">Basic AI drives</span></a>\n<ul>\n<li class=\"toclevel-2 tocsection-5\"><a href=\"#Goal-content_integrity\"><span class=\"tocnumber\">3.1</span> <span class=\"toctext\">Goal-content integrity</span></a>\n<ul>\n<li class=\"toclevel-3 tocsection-6\"><a href=\"#In_artificial_intelligence\"><span class=\"tocnumber\">3.1.1</span> <span class=\"toctext\">In artificial intelligence</span></a></li>\n</ul>\n</li>\n<li class=\"toclevel-2 tocsection-7\"><a href=\"#Resource_acquisition\"><span class=\"tocnumber\">3.2</span> <span class=\"toctext\">Resource acquisition</span></a></li>\n<li class=\"toclevel-2 tocsection-8\"><a href=\"#Cognitive_enhancement\"><span class=\"tocnumber\">3.3</span> <span class=\"toctext\">Cognitive enhancement</span></a></li>\n<li class=\"toclevel-2 tocsection-9\"><a href=\"#Technological_perfection\"><span class=\"tocnumber\">3.4</span> <span class=\"toctext\">Technological perfection</span></a></li>\n<li class=\"toclevel-2 tocsection-10\"><a href=\"#Self-preservation\"><span class=\"tocnumber\">3.5</span> <span class=\"toctext\">Self-preservation</span></a></li>\n</ul>\n</li>\n<li class=\"toclevel-1 tocsection-11\"><a href=\"#Instrumental_convergence_thesis\"><span class=\"tocnumber\">4</span> <span class=\"toctext\">Instrumental convergence thesis</span></a></li>\n<li class=\"toclevel-1 tocsection-12\"><a href=\"#Impact\"><span class=\"tocnumber\">5</span> <span class=\"toctext\">Impact</span></a></li>\n<li class=\"toclevel-1 tocsection-13\"><a href=\"#See_also\"><span class=\"tocnumber\">6</span> <span class=\"toctext\">See also</span></a></li>\n<li class=\"toclevel-1 tocsection-14\"><a href=\"#References\"><span class=\"tocnumber\">7</span> <span class=\"toctext\">References</span></a></li>\n<li class=\"toclevel-1 tocsection-15\"><a href=\"#Notes\"><span class=\"tocnumber\">8</span> <span class=\"toctext\">Notes</span></a></li>\n</ul>\n</div>\n<h2><span class=\"mw-headline\" id=\"Instrumental_and_final_goals\">Instrumental and final goals</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Instrumental_convergence&amp;action=edit&amp;section=1\" title=\"Edit section: Instrumental and final goals\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<div class=\"hatnote navigation-not-searchable\" role=\"note\">See also: <a class=\"mw-redirect\" href=\"/wiki/Instrumental_value\" title=\"Instrumental value\">Instrumental value</a></div>\n<p>Final goals, or final values, are intrinsically valuable to an intelligent agent, whether an <a href=\"/wiki/Artificial_intelligence\" title=\"Artificial intelligence\">artificial intelligence</a> or a human being, as an <a class=\"mw-redirect\" href=\"/wiki/Ends-in-themselves\" title=\"Ends-in-themselves\">end in itself</a>. In contrast, instrumental goals, or instrumental values, are only valuable to an agent as a means toward accomplishing its final goals. The contents and tradeoffs of a completely rational agent's \"final goal\" system can in principle be formalized into a <a class=\"mw-redirect\" href=\"/wiki/Utility_function\" title=\"Utility function\">utility function</a>.\n</p>\n<h2><span class=\"mw-headline\" id=\"Hypothetical_examples_of_convergence\">Hypothetical examples of convergence</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Instrumental_convergence&amp;action=edit&amp;section=2\" title=\"Edit section: Hypothetical examples of convergence\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p>One hypothetical example of instrumental convergence is provided by the <a class=\"mw-redirect\" href=\"/wiki/Riemann_Hypothesis\" title=\"Riemann Hypothesis\">Riemann Hypothesis</a> catastrophe. <a href=\"/wiki/Marvin_Minsky\" title=\"Marvin Minsky\">Marvin Minsky</a>, the co-founder of <a class=\"mw-redirect\" href=\"/wiki/MIT\" title=\"MIT\">MIT</a>'s AI laboratory, has suggested that an artificial intelligence designed to solve the Riemann hypothesis might decide to take over all of Earth's resources to build supercomputers to help achieve its goal.<sup class=\"reference\" id=\"cite_ref-aama_1-1\"><a href=\"#cite_note-aama-1\">[1]</a></sup> If the computer had instead been programmed to produce as many paper clips as possible, it would still decide to take all of Earth's resources to meet its final goal.<sup class=\"reference\" id=\"cite_ref-2\"><a href=\"#cite_note-2\">[2]</a></sup> Even though these two final goals are different, both of them produce a <i>convergent</i> instrumental goal of taking over Earth's resources.<sup class=\"reference\" id=\"cite_ref-bostrom_chapter_7_3-0\"><a href=\"#cite_note-bostrom_chapter_7-3\">[3]</a></sup>\n</p>\n<h3><span class=\"mw-headline\" id=\"Paperclip_maximizer\">Paperclip maximizer</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Instrumental_convergence&amp;action=edit&amp;section=3\" title=\"Edit section: Paperclip maximizer\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>The paperclip maximizer is a <a href=\"/wiki/Thought_experiment\" title=\"Thought experiment\">thought experiment</a> described by Swedish philosopher <a href=\"/wiki/Nick_Bostrom\" title=\"Nick Bostrom\">Nick Bostrom</a> in 2003. It illustrates the <a class=\"mw-redirect\" href=\"/wiki/Existential_risk_from_advanced_artificial_intelligence\" title=\"Existential risk from advanced artificial intelligence\">existential risk</a> that an <a href=\"/wiki/Artificial_general_intelligence\" title=\"Artificial general intelligence\">artificial general intelligence</a> may pose to human beings when programmed to pursue even seemingly-harmless goals, and the necessity of incorporating <a href=\"/wiki/Machine_ethics\" title=\"Machine ethics\">machine ethics</a> into <a href=\"/wiki/Artificial_intelligence\" title=\"Artificial intelligence\">artificial intelligence</a> design. The scenario describes an advanced artificial intelligence tasked with manufacturing paperclips. If such a machine were not programmed to value human life, or to use only designated resources in bounded time, then given enough power its optimized goal would be to turn all matter in the universe, including human beings, into either paperclips or machines which manufacture paperclips.<sup class=\"reference\" id=\"cite_ref-:0_4-0\"><a href=\"#cite_note-:0-4\">[4]</a></sup>\n</p>\n<style data-mw-deduplicate=\"TemplateStyles:r856303468\">.mw-parser-output .templatequote{overflow:hidden;margin:1em 0;padding:0 40px}.mw-parser-output .templatequote .templatequotecite{line-height:1.5em;text-align:left;padding-left:1.6em;margin-top:0}</style><blockquote class=\"templatequote\"><p>Suppose we have an AI whose only goal is to make as many paper clips as possible. The AI will realize quickly that it would be much better if there were no humans because humans might decide to switch it off. Because if humans do so, there would be fewer paper clips. Also, human bodies contain a lot of atoms that could be made into paper clips. The future that the AI would be trying to gear towards would be one in which there were a lot of paper clips but no humans.</p><div class=\"templatequotecite\">— <cite><a href=\"/wiki/Nick_Bostrom\" title=\"Nick Bostrom\">Nick Bostrom</a>, as quoted in <cite class=\"citation news\">Miles, Kathleen (2014-08-22). <a class=\"external text\" href=\"https://www.huffingtonpost.com/2014/08/22/artificial-intelligence-oxford_n_5689858.html\" rel=\"nofollow\">\"Artificial Intelligence May Doom The Human Race Within A Century, Oxford Professor Says\"</a>. <i>Huffington Post</i>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Huffington+Post&amp;rft.atitle=Artificial+Intelligence+May+Doom+The+Human+Race+Within+A+Century%2C+Oxford+Professor+Says&amp;rft.date=2014-08-22&amp;rft.aulast=Miles&amp;rft.aufirst=Kathleen&amp;rft_id=https%3A%2F%2Fwww.huffingtonpost.com%2F2014%2F08%2F22%2Fartificial-intelligence-oxford_n_5689858.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AInstrumental+convergence\"></span><style data-mw-deduplicate=\"TemplateStyles:r879151008\">.mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:\"\\\"\"\"\\\"\"\"'\"\"'\"}.mw-parser-output .citation .cs1-lock-free a{background:url(\"//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png\")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url(\"//upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Lock-gray-alt-2.svg/9px-Lock-gray-alt-2.svg.png\")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-subscription a{background:url(\"//upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Lock-red-alt-2.svg/9px-Lock-red-alt-2.svg.png\")no-repeat;background-position:right .1em center}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background:url(\"//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/12px-Wikisource-logo.svg.png\")no-repeat;background-position:right .1em center}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:inherit;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}</style><sup class=\"reference\" id=\"cite_ref-5\"><a href=\"#cite_note-5\">[5]</a></sup></cite></div>\n</blockquote><p>Bostrom has emphasised that he does not believe the paperclip maximiser scenario <i>per se</i> will actually occur; rather, his intention is to illustrate the dangers of creating <a href=\"/wiki/Superintelligence\" title=\"Superintelligence\">superintelligent</a> machines without knowing how to safely program them to eliminate existential risk to human beings.<sup class=\"reference\" id=\"cite_ref-6\"><a href=\"#cite_note-6\">[6]</a></sup> The paperclip maximizer example illustrates the broad problem of managing powerful systems that lack human values.<sup class=\"reference\" id=\"cite_ref-7\"><a href=\"#cite_note-7\">[7]</a></sup>\n</p><h2><span class=\"mw-headline\" id=\"Basic_AI_drives\">Basic AI drives</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Instrumental_convergence&amp;action=edit&amp;section=4\" title=\"Edit section: Basic AI drives\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p><a href=\"/wiki/Steve_Omohundro\" title=\"Steve Omohundro\">Steve Omohundro</a> has itemized several convergent instrumental goals, including <a href=\"/wiki/Self-preservation\" title=\"Self-preservation\">self-preservation</a> or self-protection, utility function or goal-content integrity, self-improvement, and resource acquisition. He refers to these as the \"basic AI drives\". A \"drive\" here denotes a \"tendency which will be present unless specifically counteracted\";<sup class=\"reference\" id=\"cite_ref-8\"><a href=\"#cite_note-8\">[8]</a></sup> this is different from the psychological term \"<a href=\"/wiki/Drive_theory\" title=\"Drive theory\">drive</a>\", denoting an excitatory state produced by a homeostatic disturbance.<sup class=\"reference\" id=\"cite_ref-9\"><a href=\"#cite_note-9\">[9]</a></sup> A tendency for a person to fill out income tax forms every year is a \"drive\" in Omohundro's sense, but not in the psychological sense.<sup class=\"reference\" id=\"cite_ref-10\"><a href=\"#cite_note-10\">[10]</a></sup> Daniel Dewey of the <a href=\"/wiki/Machine_Intelligence_Research_Institute\" title=\"Machine Intelligence Research Institute\">Machine Intelligence Research Institute</a> argues that even an initially introverted self-rewarding AGI may continue to acquire free energy, space, time, and freedom from interference to ensure that it will not be stopped from self-rewarding.<sup class=\"reference\" id=\"cite_ref-11\"><a href=\"#cite_note-11\">[11]</a></sup>\n</p>\n<h3><span class=\"mw-headline\" id=\"Goal-content_integrity\">Goal-content integrity</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Instrumental_convergence&amp;action=edit&amp;section=5\" title=\"Edit section: Goal-content integrity\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>In humans, maintenance of final goals can be explained with a thought experiment. Suppose a man named \"Gandhi\" has a pill that, if he took it, would cause him to want to kill people. This Gandhi is currently a pacifist: one of his explicit final goals is to never kill anyone. Gandhi is likely to refuse to take the pill, because Gandhi knows that if in the future he wants to kill people, he is likely to actually kill people, and thus the goal of \"not killing people\" would not be satisfied.<sup class=\"reference\" id=\"cite_ref-12\"><a href=\"#cite_note-12\">[12]</a></sup>\n</p><p>However, in other cases, people seem happy to let their final values drift. Humans are complicated, and their goals can be inconsistent or unknown, even to themselves.<sup class=\"reference\" id=\"cite_ref-13\"><a href=\"#cite_note-13\">[13]</a></sup>\n</p>\n<h4><span class=\"mw-headline\" id=\"In_artificial_intelligence\">In artificial intelligence</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Instrumental_convergence&amp;action=edit&amp;section=6\" title=\"Edit section: In artificial intelligence\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h4>\n<p>In 2009, <a href=\"/wiki/J%C3%BCrgen_Schmidhuber\" title=\"Jürgen Schmidhuber\">Jürgen Schmidhuber</a> concluded, in a setting where agents search for proofs about possible self-modifications, \"that any rewrites of the utility function can happen only if the Gödel machine first can prove that the rewrite is useful according to the present utility function.\"<sup class=\"reference\" id=\"cite_ref-14\"><a href=\"#cite_note-14\">[14]</a></sup><sup class=\"reference\" id=\"cite_ref-hibbard_15-0\"><a href=\"#cite_note-hibbard-15\">[15]</a></sup> An analysis by <a href=\"/wiki/Bill_Hibbard\" title=\"Bill Hibbard\">Bill Hibbard</a> of a different scenario is similarly consistent with maintenance of goal content integrity.<sup class=\"reference\" id=\"cite_ref-hibbard_15-1\"><a href=\"#cite_note-hibbard-15\">[15]</a></sup> Hibbard also argues that in a utility maximizing framework the only goal is maximizing expected utility, so that instrumental goals should be called unintended instrumental actions.<sup class=\"reference\" id=\"cite_ref-16\"><a href=\"#cite_note-16\">[16]</a></sup>\n</p>\n<h3><span class=\"mw-headline\" id=\"Resource_acquisition\">Resource acquisition</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Instrumental_convergence&amp;action=edit&amp;section=7\" title=\"Edit section: Resource acquisition\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>Many instrumental goals, such as [...] resource acquisition, are valuable to an agent because they increase its <i>freedom of action</i>.<sup class=\"reference\" id=\"cite_ref-formalizing_17-0\"><a href=\"#cite_note-formalizing-17\">[17]</a></sup><sup class=\"noprint Inline-Template\" style=\"white-space:nowrap;\">[<i><a href=\"/wiki/Wikipedia:Citing_sources#What_information_to_include\" title=\"Wikipedia:Citing sources\"><span title=\"A complete citation is needed (September 2018)\">full citation needed</span></a></i>]</sup>\n</p><p>For almost any open-ended, non-trivial reward function (or set of goals), possessing more resources (such as equipment, raw materials, or energy) can enable the AI to find a more \"optimal\" solution. Resources can benefit some AIs directly, through being able to create more of whatever stuff its reward function values: \"The AI neither hates you, nor loves you, but you are made out of atoms that it can use for something else.\"<sup class=\"reference\" id=\"cite_ref-18\"><a href=\"#cite_note-18\">[18]</a></sup><sup class=\"reference\" id=\"cite_ref-shanahan_7.5_19-0\"><a href=\"#cite_note-shanahan_7.5-19\">[19]</a></sup> In addition, almost all AIs can benefit from having more resources to spend on other instrumental goals, such as self-preservation.<sup class=\"reference\" id=\"cite_ref-shanahan_7.5_19-1\"><a href=\"#cite_note-shanahan_7.5-19\">[19]</a></sup>\n</p>\n<h3><span class=\"mw-headline\" id=\"Cognitive_enhancement\">Cognitive enhancement</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Instrumental_convergence&amp;action=edit&amp;section=8\" title=\"Edit section: Cognitive enhancement\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>\"If the agent's final goals are fairly unbounded and the agent is in a position to become the first superintelligence and thereby obtain a decisive strategic advantage, [...] according to its preferences. At least in this special case, a rational intelligent agent would place a very *high instrumental value on cognitive enhancement*\" <sup class=\"reference\" id=\"cite_ref-super_20-0\"><a href=\"#cite_note-super-20\">[20]</a></sup><sup class=\"noprint Inline-Template\" style=\"white-space:nowrap;\">[<i><a href=\"/wiki/Wikipedia:Citing_sources\" title=\"Wikipedia:Citing sources\"><span title=\"This citation requires a reference to the specific page or range of pages in which the material appears. (September 2018)\">page needed</span></a></i>]</sup>\n</p>\n<h3><span class=\"mw-headline\" id=\"Technological_perfection\">Technological perfection</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Instrumental_convergence&amp;action=edit&amp;section=9\" title=\"Edit section: Technological perfection\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>Many instrumental goals, such as [...] technological advancement, are valuable to an agent because they increase its <i>freedom of action</i>.<sup class=\"reference\" id=\"cite_ref-formalizing_17-1\"><a href=\"#cite_note-formalizing-17\">[17]</a></sup><sup class=\"noprint Inline-Template\" style=\"white-space:nowrap;\">[<i><a href=\"/wiki/Wikipedia:Citing_sources#What_information_to_include\" title=\"Wikipedia:Citing sources\"><span title=\"A complete citation is needed (September 2018)\">full citation needed</span></a></i>]</sup>\n</p>\n<h3><span class=\"mw-headline\" id=\"Self-preservation\">Self-preservation</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Instrumental_convergence&amp;action=edit&amp;section=10\" title=\"Edit section: Self-preservation\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>Many instrumental goals, such as [...] self-preservation, are valuable to an agent because they increase its <i>freedom of action</i>.<sup class=\"reference\" id=\"cite_ref-formalizing_17-2\"><a href=\"#cite_note-formalizing-17\">[17]</a></sup><sup class=\"noprint Inline-Template\" style=\"white-space:nowrap;\">[<i><a href=\"/wiki/Wikipedia:Citing_sources#What_information_to_include\" title=\"Wikipedia:Citing sources\"><span title=\"A complete citation is needed (September 2018)\">full citation needed</span></a></i>]</sup>\n</p>\n<h2><span class=\"mw-headline\" id=\"Instrumental_convergence_thesis\">Instrumental convergence thesis</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Instrumental_convergence&amp;action=edit&amp;section=11\" title=\"Edit section: Instrumental convergence thesis\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p>The instrumental convergence thesis, as outlined by philosopher <a href=\"/wiki/Nick_Bostrom\" title=\"Nick Bostrom\">Nick Bostrom</a>, states:\n</p>\n<blockquote><p>Several instrumental values can be identified which are convergent in the sense that their attainment would increase the chances of the agent's goal being realized for a wide range of final goals and a wide range of situations, implying that these instrumental values are likely to be pursued by a broad spectrum of situated intelligent agents.</p></blockquote>\n<p>The instrumental convergence thesis applies only to instrumental goals; intelligent agents may have a wide variety of possible final goals.<sup class=\"reference\" id=\"cite_ref-bostrom_chapter_7_3-1\"><a href=\"#cite_note-bostrom_chapter_7-3\">[3]</a></sup> Note that by Bostrom's <a class=\"new\" href=\"/w/index.php?title=Orthogonality_Thesis&amp;action=edit&amp;redlink=1\" title=\"Orthogonality Thesis (page does not exist)\">Orthogonality Thesis</a>,<sup class=\"reference\" id=\"cite_ref-bostrom_chapter_7_3-2\"><a href=\"#cite_note-bostrom_chapter_7-3\">[3]</a></sup> final goals of highly intelligent agents may be well-bounded in space, time, and resources; well-bounded ultimate goals do not, in general, engender unbounded instrumental goals.<sup class=\"reference\" id=\"cite_ref-21\"><a href=\"#cite_note-21\">[21]</a></sup>\n</p>\n<h2><span class=\"mw-headline\" id=\"Impact\">Impact</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Instrumental_convergence&amp;action=edit&amp;section=12\" title=\"Edit section: Impact\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p>Agents can acquire resources by trade or by conquest. A rational agent will, by definition, choose whatever option will maximize its implicit utility function; therefore a rational agent will trade for a subset of another agent's resources only if outright seizing the resources is too risky or costly (compared with the gains from taking all the resources), or if some other element in its utility function bars it from the seizure. In the case of a powerful, self-interested, rational superintelligence interacting with a lesser intelligence, peaceful trade (rather than unilateral seizure) seems unnecessary and suboptimal, and therefore unlikely.<sup class=\"reference\" id=\"cite_ref-formalizing_17-3\"><a href=\"#cite_note-formalizing-17\">[17]</a></sup><sup class=\"noprint Inline-Template\" style=\"white-space:nowrap;\">[<i><a href=\"/wiki/Wikipedia:Citing_sources#What_information_to_include\" title=\"Wikipedia:Citing sources\"><span title=\"A complete citation is needed (September 2018)\">full citation needed</span></a></i>]</sup>\n</p><p>Some observers, such as Skype's <a href=\"/wiki/Jaan_Tallinn\" title=\"Jaan Tallinn\">Jaan Tallinn</a> and physicist <a href=\"/wiki/Max_Tegmark\" title=\"Max Tegmark\">Max Tegmark</a>, believe that \"basic AI drives\", and other <a href=\"/wiki/Unintended_consequences\" title=\"Unintended consequences\">unintended consequences</a> of superintelligent AI programmed by well-meaning programmers, could pose a significant threat to <a class=\"mw-redirect\" href=\"/wiki/Human_survival\" title=\"Human survival\">human survival</a>, especially if an \"intelligence explosion\" abruptly occurs due to <a class=\"mw-redirect\" href=\"/wiki/Recursive\" title=\"Recursive\">recursive</a> self-improvement. Since nobody knows how to predict beforehand when <a href=\"/wiki/Superintelligence\" title=\"Superintelligence\">superintelligence</a> will arrive, such observers call for research into <a href=\"/wiki/Friendly_artificial_intelligence\" title=\"Friendly artificial intelligence\">friendly artificial intelligence</a> as a possible way to mitigate <a href=\"/wiki/Existential_risk_from_artificial_general_intelligence\" title=\"Existential risk from artificial general intelligence\">existential risk from artificial general intelligence</a>.<sup class=\"reference\" id=\"cite_ref-22\"><a href=\"#cite_note-22\">[22]</a></sup>\n</p>\n<h2><span class=\"mw-headline\" id=\"See_also\">See also</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Instrumental_convergence&amp;action=edit&amp;section=13\" title=\"Edit section: See also\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<ul><li><a class=\"mw-redirect\" href=\"/wiki/Instrumental_value\" title=\"Instrumental value\">Instrumental value</a></li>\n<li><a class=\"mw-redirect\" href=\"/wiki/Intrinsic_value_(ethics)\" title=\"Intrinsic value (ethics)\">Intrinsic value (ethics)</a></li>\n<li><a href=\"/wiki/Friendly_artificial_intelligence\" title=\"Friendly artificial intelligence\">Friendly artificial intelligence</a></li></ul>\n<h2><span class=\"mw-headline\" id=\"References\">References</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Instrumental_convergence&amp;action=edit&amp;section=14\" title=\"Edit section: References\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<ul><li><a href=\"/wiki/Nick_Bostrom\" title=\"Nick Bostrom\">Nick Bostrom</a> (2014). <i><a href=\"/wiki/Superintelligence:_Paths,_Dangers,_Strategies\" title=\"Superintelligence: Paths, Dangers, Strategies\">Superintelligence: Paths, Dangers, Strategies</a></i>. Oxford: Oxford University Press. <link href=\"mw-data:TemplateStyles:r879151008\" rel=\"mw-deduplicated-inline-style\"/><a href=\"/wiki/International_Standard_Book_Number\" title=\"International Standard Book Number\">ISBN</a> <a href=\"/wiki/Special:BookSources/9780199678112\" title=\"Special:BookSources/9780199678112\">9780199678112</a>.</li></ul>\n<h2><span class=\"mw-headline\" id=\"Notes\">Notes</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Instrumental_convergence&amp;action=edit&amp;section=15\" title=\"Edit section: Notes\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<div class=\"reflist columns references-column-width\" style=\"-moz-column-width: 30em; -webkit-column-width: 30em; column-width: 30em; list-style-type: decimal;\">\n<ol class=\"references\">\n<li id=\"cite_note-aama-1\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-aama_1-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-aama_1-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\"><cite class=\"citation book\">Russell, Stuart J.; Norvig, Peter (2003). \"Section 26.3: The Ethics and Risks of Developing Artificial Intelligence\". <a href=\"/wiki/Artificial_Intelligence:_A_Modern_Approach\" title=\"Artificial Intelligence: A Modern Approach\"><i>Artificial Intelligence: A Modern Approach</i></a>. Upper Saddle River, N.J.: Prentice Hall. <a href=\"/wiki/International_Standard_Book_Number\" title=\"International Standard Book Number\">ISBN</a> <a href=\"/wiki/Special:BookSources/978-0137903955\" title=\"Special:BookSources/978-0137903955\">978-0137903955</a>. <q>Similarly, Marvin Minsky once suggested that an AI program designed to solve the Riemann Hypothesis might end up taking over all the resources of Earth to build more powerful supercomputers to help achieve its goal.</q></cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Section+26.3%3A+The+Ethics+and+Risks+of+Developing+Artificial+Intelligence&amp;rft.btitle=Artificial+Intelligence%3A+A+Modern+Approach&amp;rft.place=Upper+Saddle+River%2C+N.J.&amp;rft.pub=Prentice+Hall&amp;rft.date=2003&amp;rft.isbn=978-0137903955&amp;rft.aulast=Russell&amp;rft.aufirst=Stuart+J.&amp;rft.au=Norvig%2C+Peter&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AInstrumental+convergence\"></span><link href=\"mw-data:TemplateStyles:r879151008\" rel=\"mw-deduplicated-inline-style\"/></span>\n</li>\n<li id=\"cite_note-2\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-2\">^</a></b></span> <span class=\"reference-text\">Bostrom 2014, Chapter 8, p. 123. \"An AI, designed to manage production in a factory, is given the final goal of maximizing the manufacturing of paperclips, and proceeds by converting first the Earth and then increasingly large chunks of the observable universe into paperclips.\"</span>\n</li>\n<li id=\"cite_note-bostrom_chapter_7-3\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-bostrom_chapter_7_3-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-bostrom_chapter_7_3-1\"><sup><i><b>b</b></i></sup></a> <a href=\"#cite_ref-bostrom_chapter_7_3-2\"><sup><i><b>c</b></i></sup></a></span> <span class=\"reference-text\">Bostrom 2014, chapter 7.</span>\n</li>\n<li id=\"cite_note-:0-4\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-:0_4-0\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation web\">Bostrom, Nick (2003). <a class=\"external text\" href=\"http://www.nickbostrom.com/ethics/ai.html\" rel=\"nofollow\">\"Ethical Issues in Advanced Artificial Intelligence\"</a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Ethical+Issues+in+Advanced+Artificial+Intelligence&amp;rft.date=2003&amp;rft.aulast=Bostrom&amp;rft.aufirst=Nick&amp;rft_id=http%3A%2F%2Fwww.nickbostrom.com%2Fethics%2Fai.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AInstrumental+convergence\"></span><link href=\"mw-data:TemplateStyles:r879151008\" rel=\"mw-deduplicated-inline-style\"/></span>\n</li>\n<li id=\"cite_note-5\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-5\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation news\">Miles, Kathleen (2014-08-22). <a class=\"external text\" href=\"https://www.huffingtonpost.com/2014/08/22/artificial-intelligence-oxford_n_5689858.html\" rel=\"nofollow\">\"Artificial Intelligence May Doom The Human Race Within A Century, Oxford Professor Says\"</a>. <i>Huffington Post</i>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Huffington+Post&amp;rft.atitle=Artificial+Intelligence+May+Doom+The+Human+Race+Within+A+Century%2C+Oxford+Professor+Says&amp;rft.date=2014-08-22&amp;rft.aulast=Miles&amp;rft.aufirst=Kathleen&amp;rft_id=https%3A%2F%2Fwww.huffingtonpost.com%2F2014%2F08%2F22%2Fartificial-intelligence-oxford_n_5689858.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AInstrumental+convergence\"></span><link href=\"mw-data:TemplateStyles:r879151008\" rel=\"mw-deduplicated-inline-style\"/></span>\n</li>\n<li id=\"cite_note-6\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-6\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation web\">Ford, Paul (11 February 2015). <a class=\"external text\" href=\"http://www.technologyreview.com/review/534871/our-fear-of-artificial-intelligence/\" rel=\"nofollow\">\"Are We Smart Enough to Control Artificial Intelligence?\"</a>. <i>MIT Technology Review</i><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">25 January</span> 2016</span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=MIT+Technology+Review&amp;rft.atitle=Are+We+Smart+Enough+to+Control+Artificial+Intelligence%3F&amp;rft.date=2015-02-11&amp;rft.aulast=Ford&amp;rft.aufirst=Paul&amp;rft_id=http%3A%2F%2Fwww.technologyreview.com%2Freview%2F534871%2Four-fear-of-artificial-intelligence%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AInstrumental+convergence\"></span><link href=\"mw-data:TemplateStyles:r879151008\" rel=\"mw-deduplicated-inline-style\"/></span>\n</li>\n<li id=\"cite_note-7\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-7\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation news\">Friend, Tad (3 October 2016). <a class=\"external text\" href=\"https://www.newyorker.com/magazine/2016/10/10/sam-altmans-manifest-destiny\" rel=\"nofollow\">\"Sam Altman's Manifest Destiny\"</a>. <i>The New Yorker</i><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">25 November</span> 2017</span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+New+Yorker&amp;rft.atitle=Sam+Altman%E2%80%99s+Manifest+Destiny&amp;rft.date=2016-10-03&amp;rft.aulast=Friend&amp;rft.aufirst=Tad&amp;rft_id=https%3A%2F%2Fwww.newyorker.com%2Fmagazine%2F2016%2F10%2F10%2Fsam-altmans-manifest-destiny&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AInstrumental+convergence\"></span><link href=\"mw-data:TemplateStyles:r879151008\" rel=\"mw-deduplicated-inline-style\"/></span>\n</li>\n<li id=\"cite_note-8\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-8\">^</a></b></span> <span class=\"reference-text\">Omohundro, S. M. (2008, February). The basic AI drives. In AGI (Vol. 171, pp. 483-492).</span>\n</li>\n<li id=\"cite_note-9\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-9\">^</a></b></span> <span class=\"reference-text\">Seward, J. (1956). Drive, incentive, and reinforcement. Psychological Review, 63, 19-203.</span>\n</li>\n<li id=\"cite_note-10\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-10\">^</a></b></span> <span class=\"reference-text\">Bostrom 2014, footnote 8 to chapter 7.</span>\n</li>\n<li id=\"cite_note-11\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-11\">^</a></b></span> <span class=\"reference-text\">Dewey, Daniel. \"Learning what to value.\" Artificial General Intelligence (2011): 309-314.</span>\n</li>\n<li id=\"cite_note-12\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-12\">^</a></b></span> <span class=\"reference-text\">Yudkowsky, Eliezer. \"Complex value systems in friendly AI.\" In Artificial general intelligence, pp. 388-393. Springer Berlin Heidelberg, 2011.</span>\n</li>\n<li id=\"cite_note-13\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-13\">^</a></b></span> <span class=\"reference-text\">Bostrom 2014, chapter 7, p. 110. \"We humans often seem happy to let our final values drift... For example, somebody deciding to have a child might predict that they will come to value the child for its own sake, even though at the time of the decision they may not particularly value their future child... Humans are complicated, and many factors might be in play in a situation like this... one might have a final value that involves having certain experiences and occupying a certain social role; and become a parent— and undergoing the attendant goal shift— might be a necessary aspect of that...\"</span>\n</li>\n<li id=\"cite_note-14\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-14\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Schmidhuber, J. R. (2009). \"Ultimate Cognition à la Gödel\". <i>Cognitive Computation</i>. <b>1</b> (2): 177–193. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a class=\"external text\" href=\"//doi.org/10.1007%2Fs12559-009-9014-y\" rel=\"nofollow\">10.1007/s12559-009-9014-y</a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Cognitive+Computation&amp;rft.atitle=Ultimate+Cognition+%C3%A0+la+G%C3%B6del&amp;rft.volume=1&amp;rft.issue=2&amp;rft.pages=177-193&amp;rft.date=2009&amp;rft_id=info%3Adoi%2F10.1007%2Fs12559-009-9014-y&amp;rft.aulast=Schmidhuber&amp;rft.aufirst=J.+R.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AInstrumental+convergence\"></span><link href=\"mw-data:TemplateStyles:r879151008\" rel=\"mw-deduplicated-inline-style\"/></span>\n</li>\n<li id=\"cite_note-hibbard-15\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-hibbard_15-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-hibbard_15-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\"><cite class=\"citation journal\">Hibbard, B. (2012). \"Model-based Utility Functions\". <i>Journal of Artificial General Intelligence</i>. <b>3</b>: 1–24. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a class=\"external text\" href=\"//doi.org/10.2478%2Fv10229-011-0013-5\" rel=\"nofollow\">10.2478/v10229-011-0013-5</a>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Artificial+General+Intelligence&amp;rft.atitle=Model-based+Utility+Functions&amp;rft.volume=3&amp;rft.pages=1-24&amp;rft.date=2012&amp;rft_id=info%3Adoi%2F10.2478%2Fv10229-011-0013-5&amp;rft.aulast=Hibbard&amp;rft.aufirst=B.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AInstrumental+convergence\"></span><link href=\"mw-data:TemplateStyles:r879151008\" rel=\"mw-deduplicated-inline-style\"/></span>\n</li>\n<li id=\"cite_note-16\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-16\">^</a></b></span> <span class=\"reference-text\">Hibbard, Bill (2014): Ethical Artificial Intelligence. <a class=\"external free\" href=\"https://arxiv.org/abs/1411.1373\" rel=\"nofollow\">https://arxiv.org/abs/1411.1373</a></span>\n</li>\n<li id=\"cite_note-formalizing-17\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-formalizing_17-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-formalizing_17-1\"><sup><i><b>b</b></i></sup></a> <a href=\"#cite_ref-formalizing_17-2\"><sup><i><b>c</b></i></sup></a> <a href=\"#cite_ref-formalizing_17-3\"><sup><i><b>d</b></i></sup></a></span> <span class=\"reference-text\">Benson-Tilsen, T., &amp; Soares, N. (2016, March). Formalizing Convergent Instrumental Goals. In AAAI Workshop: AI, Ethics, and Society.</span>\n</li>\n<li id=\"cite_note-18\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-18\">^</a></b></span> <span class=\"reference-text\">Yudkowsky, Eliezer. \"Artificial intelligence as a positive and negative factor in global risk.\" Global catastrophic risks (2008): 303. p. 333.</span>\n</li>\n<li id=\"cite_note-shanahan_7.5-19\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-shanahan_7.5_19-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-shanahan_7.5_19-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\"><a href=\"/wiki/Murray_Shanahan\" title=\"Murray Shanahan\">Murray Shanahan</a>. <i>The Technological Singularity</i>. MIT Press, 2015. Chapter 7, Section 5: \"Safe Superintelligence\".</span>\n</li>\n<li id=\"cite_note-super-20\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-super_20-0\">^</a></b></span> <span class=\"reference-text\">Bostrom, N. (2016). Superintelligence, Oxford University Press</span>\n</li>\n<li id=\"cite_note-21\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-21\">^</a></b></span> <span class=\"reference-text\"><a class=\"external text\" href=\"https://www.fhi.ox.ac.uk/wp-content/uploads/Reframing_Superintelligence_FHI-TR-2019-1.1-1.pdf\" rel=\"nofollow\">Reframing Superintelligence: Comprehensive AI Services as General Intelligence, Technical Report, 2019</a>, Future of Humanity Institute</span>\n</li>\n<li id=\"cite_note-22\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-22\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation news\"><a class=\"external text\" href=\"https://www.chronicle.com/article/Is-Artificial-Intelligence-a/148763\" rel=\"nofollow\">\"Is Artificial Intelligence a Threat?\"</a>. <i><a href=\"/wiki/The_Chronicle_of_Higher_Education\" title=\"The Chronicle of Higher Education\">The Chronicle of Higher Education</a></i>. 11 September 2014<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">25 November</span> 2017</span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+Chronicle+of+Higher+Education&amp;rft.atitle=Is+Artificial+Intelligence+a+Threat%3F&amp;rft.date=2014-09-11&amp;rft_id=https%3A%2F%2Fwww.chronicle.com%2Farticle%2FIs-Artificial-Intelligence-a%2F148763&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AInstrumental+convergence\"></span><link href=\"mw-data:TemplateStyles:r879151008\" rel=\"mw-deduplicated-inline-style\"/></span>\n</li>\n</ol></div>\n<div aria-labelledby=\"Risks_from_artificial_intelligence\" class=\"navbox\" role=\"navigation\" style=\"padding:3px\"><table class=\"nowraplinks collapsible autocollapse navbox-inner\" style=\"border-spacing:0;background:transparent;color:inherit\"><tbody><tr><th class=\"navbox-title\" colspan=\"2\" scope=\"col\"><div class=\"plainlinks hlist navbar mini\"><ul><li class=\"nv-view\"><a href=\"/wiki/Template:Existential_risk_from_artificial_intelligence\" title=\"Template:Existential risk from artificial intelligence\"><abbr style=\";;background:none transparent;border:none;-moz-box-shadow:none;-webkit-box-shadow:none;box-shadow:none; padding:0;\" title=\"View this template\">v</abbr></a></li><li class=\"nv-talk\"><a href=\"/wiki/Template_talk:Existential_risk_from_artificial_intelligence\" title=\"Template talk:Existential risk from artificial intelligence\"><abbr style=\";;background:none transparent;border:none;-moz-box-shadow:none;-webkit-box-shadow:none;box-shadow:none; padding:0;\" title=\"Discuss this template\">t</abbr></a></li><li class=\"nv-edit\"><a class=\"external text\" href=\"//en.wikipedia.org/w/index.php?title=Template:Existential_risk_from_artificial_intelligence&amp;action=edit\"><abbr style=\";;background:none transparent;border:none;-moz-box-shadow:none;-webkit-box-shadow:none;box-shadow:none; padding:0;\" title=\"Edit this template\">e</abbr></a></li></ul></div><div id=\"Risks_from_artificial_intelligence\" style=\"font-size:114%;margin:0 4em\">Risks from <a href=\"/wiki/Artificial_intelligence\" title=\"Artificial intelligence\">artificial intelligence</a></div></th></tr><tr><th class=\"navbox-group\" scope=\"row\" style=\"width:1%\">Concepts</th><td class=\"navbox-list navbox-odd hlist\" style=\"text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px\"><div style=\"padding:0em 0.25em\">\n<ul><li><a href=\"/wiki/AI_box\" title=\"AI box\">AI box</a></li>\n<li><a href=\"/wiki/AI_takeover\" title=\"AI takeover\">AI takeover</a></li>\n<li><a href=\"/wiki/AI_control_problem\" title=\"AI control problem\">Control problem</a></li>\n<li><a href=\"/wiki/Existential_risk_from_artificial_general_intelligence\" title=\"Existential risk from artificial general intelligence\">Existential risk from artificial general intelligence</a></li>\n<li><a href=\"/wiki/Friendly_artificial_intelligence\" title=\"Friendly artificial intelligence\">Friendly artificial intelligence</a></li>\n<li><a class=\"mw-selflink selflink\">Instrumental convergence</a></li>\n<li><a class=\"mw-redirect\" href=\"/wiki/Intelligence_explosion\" title=\"Intelligence explosion\">Intelligence explosion</a></li>\n<li><a href=\"/wiki/Machine_ethics\" title=\"Machine ethics\">Machine ethics</a></li>\n<li><a href=\"/wiki/Superintelligence\" title=\"Superintelligence\">Superintelligence</a></li>\n<li><a href=\"/wiki/Technological_singularity\" title=\"Technological singularity\">Technological singularity</a></li></ul>\n</div></td></tr><tr><th class=\"navbox-group\" scope=\"row\" style=\"width:1%\">Organizations</th><td class=\"navbox-list navbox-even hlist\" style=\"text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px\"><div style=\"padding:0em 0.25em\">\n<ul><li><a href=\"/wiki/Allen_Institute_for_Artificial_Intelligence\" title=\"Allen Institute for Artificial Intelligence\">Allen Institute for Artificial Intelligence</a></li>\n<li><a href=\"/wiki/Center_for_Applied_Rationality\" title=\"Center for Applied Rationality\">Center for Applied Rationality</a></li>\n<li><a href=\"/wiki/Centre_for_the_Study_of_Existential_Risk\" title=\"Centre for the Study of Existential Risk\">Centre for the Study of Existential Risk</a></li>\n<li><a href=\"/wiki/DeepMind\" title=\"DeepMind\">DeepMind</a></li>\n<li><a href=\"/wiki/Foundational_Questions_Institute\" title=\"Foundational Questions Institute\">Foundational Questions Institute</a></li>\n<li><a href=\"/wiki/Future_of_Humanity_Institute\" title=\"Future of Humanity Institute\">Future of Humanity Institute</a></li>\n<li><a href=\"/wiki/Future_of_Life_Institute\" title=\"Future of Life Institute\">Future of Life Institute</a></li>\n<li><a href=\"/wiki/Humanity%2B\" title=\"Humanity+\">Humanity+</a></li>\n<li><a href=\"/wiki/Institute_for_Ethics_and_Emerging_Technologies\" title=\"Institute for Ethics and Emerging Technologies\">Institute for Ethics and Emerging Technologies</a></li>\n<li><a href=\"/wiki/Leverhulme_Centre_for_the_Future_of_Intelligence\" title=\"Leverhulme Centre for the Future of Intelligence\">Leverhulme Centre for the Future of Intelligence</a></li>\n<li><a href=\"/wiki/Machine_Intelligence_Research_Institute\" title=\"Machine Intelligence Research Institute\">Machine Intelligence Research Institute</a></li>\n<li><a href=\"/wiki/OpenAI\" title=\"OpenAI\">OpenAI</a></li></ul>\n</div></td></tr><tr><th class=\"navbox-group\" scope=\"row\" style=\"width:1%\">People</th><td class=\"navbox-list navbox-odd hlist\" style=\"text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px\"><div style=\"padding:0em 0.25em\">\n<ul><li><a href=\"/wiki/Nick_Bostrom\" title=\"Nick Bostrom\">Nick Bostrom</a></li>\n<li><a href=\"/wiki/Stephen_Hawking\" title=\"Stephen Hawking\">Stephen Hawking</a></li>\n<li><a href=\"/wiki/Bill_Hibbard\" title=\"Bill Hibbard\">Bill Hibbard</a></li>\n<li><a href=\"/wiki/Bill_Joy\" title=\"Bill Joy\">Bill Joy</a></li>\n<li><a href=\"/wiki/Elon_Musk\" title=\"Elon Musk\">Elon Musk</a></li>\n<li><a href=\"/wiki/Steve_Omohundro\" title=\"Steve Omohundro\">Steve Omohundro</a></li>\n<li><a href=\"/wiki/Huw_Price\" title=\"Huw Price\">Huw Price</a></li>\n<li><a href=\"/wiki/Martin_Rees\" title=\"Martin Rees\">Martin Rees</a></li>\n<li><a href=\"/wiki/Stuart_J._Russell\" title=\"Stuart J. Russell\">Stuart J. Russell</a></li>\n<li><a href=\"/wiki/Jaan_Tallinn\" title=\"Jaan Tallinn\">Jaan Tallinn</a></li>\n<li><a href=\"/wiki/Max_Tegmark\" title=\"Max Tegmark\">Max Tegmark</a></li>\n<li><a href=\"/wiki/Frank_Wilczek\" title=\"Frank Wilczek\">Frank Wilczek</a></li>\n<li><a href=\"/wiki/Roman_Yampolskiy\" title=\"Roman Yampolskiy\">Roman Yampolskiy</a></li>\n<li><a href=\"/wiki/Eliezer_Yudkowsky\" title=\"Eliezer Yudkowsky\">Eliezer Yudkowsky</a></li>\n<li><a href=\"/wiki/Sam_Harris\" title=\"Sam Harris\">Sam Harris</a></li></ul>\n</div></td></tr><tr><th class=\"navbox-group\" scope=\"row\" style=\"width:1%\">Other</th><td class=\"navbox-list navbox-even hlist\" style=\"text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px\"><div style=\"padding:0em 0.25em\">\n<ul><li><a href=\"/wiki/Open_Letter_on_Artificial_Intelligence\" title=\"Open Letter on Artificial Intelligence\">Open Letter on Artificial Intelligence</a></li>\n<li><a href=\"/wiki/Ethics_of_artificial_intelligence\" title=\"Ethics of artificial intelligence\">Ethics of artificial intelligence</a></li>\n<li><a href=\"/wiki/Artificial_general_intelligence#Controversies_and_dangers\" title=\"Artificial general intelligence\">Controversies and dangers of artificial general intelligence</a></li>\n<li><a href=\"/wiki/Global_catastrophic_risk#Artificial_intelligence\" title=\"Global catastrophic risk\">Artificial intelligence as a global catastrophic risk</a></li>\n<li><i><a href=\"/wiki/Superintelligence:_Paths,_Dangers,_Strategies\" title=\"Superintelligence: Paths, Dangers, Strategies\">Superintelligence: Paths, Dangers, Strategies</a></i></li>\n<li><i><a href=\"/wiki/Our_Final_Invention\" title=\"Our Final Invention\">Our Final Invention</a></i></li></ul>\n</div></td></tr></tbody></table></div>\n<!-- \nNewPP limit report\nParsed by mw1281\nCached time: 20190222152402\nCache expiry: 2073600\nDynamic content: false\nCPU time usage: 0.344 seconds\nReal time usage: 0.462 seconds\nPreprocessor visited node count: 1584/1000000\nPreprocessor generated node count: 0/1500000\nPost‐expand include size: 42893/2097152 bytes\nTemplate argument size: 5546/2097152 bytes\nHighest expansion depth: 12/40\nExpensive parser function count: 3/500\nUnstrip recursion depth: 1/20\nUnstrip post‐expand size: 34828/5000000 bytes\nNumber of Wikibase entities loaded: 1/400\nLua time usage: 0.169/10.000 seconds\nLua memory usage: 4.51 MB/50 MB\n-->\n<!--\nTransclusion expansion time report (%,ms,calls,template)\n100.00%  394.629      1 -total\n 25.28%   99.779      1 Template:Quote\n 23.59%   93.095      1 Template:Reflist\n 21.22%   83.751      5 Template:Fix\n 20.83%   82.204      4 Template:Cite_news\n 20.14%   79.482      4 Template:Fcn\n 10.49%   41.391      6 Template:Category_handler\n 10.06%   39.718      1 Template:ISBN\n  9.21%   36.327      5 Template:Delink\n  9.09%   35.865      2 Template:Cite_journal\n-->\n<!-- Saved in parser cache with key enwiki:pcache:idhash:43591208-0!canonical and timestamp 20190222152402 and revision id 884577623\n -->\n</div><noscript><img alt=\"\" height=\"1\" src=\"//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1\" style=\"border: none; position: absolute;\" title=\"\" width=\"1\"/></noscript></div> <div class=\"printfooter\">\n\t\t\t\t\t\tRetrieved from \"<a dir=\"ltr\" href=\"https://en.wikipedia.org/w/index.php?title=Instrumental_convergence&amp;oldid=884577623\">https://en.wikipedia.org/w/index.php?title=Instrumental_convergence&amp;oldid=884577623</a>\"\t\t\t\t\t</div>\n<div class=\"catlinks\" data-mw=\"interface\" id=\"catlinks\"><div class=\"mw-normal-catlinks\" id=\"mw-normal-catlinks\"><a href=\"/wiki/Help:Category\" title=\"Help:Category\">Categories</a>: <ul><li><a href=\"/wiki/Category:Artificial_intelligence\" title=\"Category:Artificial intelligence\">Artificial intelligence</a></li><li><a href=\"/wiki/Category:Goal\" title=\"Category:Goal\">Goal</a></li><li><a href=\"/wiki/Category:Intention\" title=\"Category:Intention\">Intention</a></li><li><a href=\"/wiki/Category:Risk\" title=\"Category:Risk\">Risk</a></li><li><a href=\"/wiki/Category:Artificial_Intelligence_existential_risk\" title=\"Category:Artificial Intelligence existential risk\">Artificial Intelligence existential risk</a></li></ul></div><div class=\"mw-hidden-catlinks mw-hidden-cats-hidden\" id=\"mw-hidden-catlinks\">Hidden categories: <ul><li><a href=\"/wiki/Category:Articles_with_incomplete_citations_from_September_2018\" title=\"Category:Articles with incomplete citations from September 2018\">Articles with incomplete citations from September 2018</a></li><li><a href=\"/wiki/Category:All_articles_with_incomplete_citations\" title=\"Category:All articles with incomplete citations\">All articles with incomplete citations</a></li><li><a href=\"/wiki/Category:Wikipedia_articles_needing_page_number_citations_from_September_2018\" title=\"Category:Wikipedia articles needing page number citations from September 2018\">Wikipedia articles needing page number citations from September 2018</a></li></ul></div></div> <div class=\"visualClear\"></div>\n</div>\n</div>\n<div id=\"mw-navigation\">\n<h2>Navigation menu</h2>\n<div id=\"mw-head\">\n<div aria-labelledby=\"p-personal-label\" id=\"p-personal\" role=\"navigation\">\n<h3 id=\"p-personal-label\">Personal tools</h3>\n<ul>\n<li id=\"pt-anonuserpage\">Not logged in</li><li id=\"pt-anontalk\"><a accesskey=\"n\" href=\"/wiki/Special:MyTalk\" title=\"Discussion about edits from this IP address [n]\">Talk</a></li><li id=\"pt-anoncontribs\"><a accesskey=\"y\" href=\"/wiki/Special:MyContributions\" title=\"A list of edits made from this IP address [y]\">Contributions</a></li><li id=\"pt-createaccount\"><a href=\"/w/index.php?title=Special:CreateAccount&amp;returnto=Instrumental+convergence\" title=\"You are encouraged to create an account and log in; however, it is not mandatory\">Create account</a></li><li id=\"pt-login\"><a accesskey=\"o\" href=\"/w/index.php?title=Special:UserLogin&amp;returnto=Instrumental+convergence\" title=\"You're encouraged to log in; however, it's not mandatory. [o]\">Log in</a></li> </ul>\n</div>\n<div id=\"left-navigation\">\n<div aria-labelledby=\"p-namespaces-label\" class=\"vectorTabs\" id=\"p-namespaces\" role=\"navigation\">\n<h3 id=\"p-namespaces-label\">Namespaces</h3>\n<ul>\n<li class=\"selected\" id=\"ca-nstab-main\"><span><a accesskey=\"c\" href=\"/wiki/Instrumental_convergence\" title=\"View the content page [c]\">Article</a></span></li><li id=\"ca-talk\"><span><a accesskey=\"t\" href=\"/wiki/Talk:Instrumental_convergence\" rel=\"discussion\" title=\"Discussion about the content page [t]\">Talk</a></span></li> </ul>\n</div>\n<div aria-labelledby=\"p-variants-label\" class=\"vectorMenu emptyPortlet\" id=\"p-variants\" role=\"navigation\">\n<input aria-labelledby=\"p-variants-label\" class=\"vectorMenuCheckbox\" type=\"checkbox\"/>\n<h3 id=\"p-variants-label\">\n<span>Variants</span>\n</h3>\n<ul class=\"menu\">\n</ul>\n</div>\n</div>\n<div id=\"right-navigation\">\n<div aria-labelledby=\"p-views-label\" class=\"vectorTabs\" id=\"p-views\" role=\"navigation\">\n<h3 id=\"p-views-label\">Views</h3>\n<ul>\n<li class=\"collapsible selected\" id=\"ca-view\"><span><a href=\"/wiki/Instrumental_convergence\">Read</a></span></li><li class=\"collapsible\" id=\"ca-edit\"><span><a accesskey=\"e\" href=\"/w/index.php?title=Instrumental_convergence&amp;action=edit\" title=\"Edit this page [e]\">Edit</a></span></li><li class=\"collapsible\" id=\"ca-history\"><span><a accesskey=\"h\" href=\"/w/index.php?title=Instrumental_convergence&amp;action=history\" title=\"Past revisions of this page [h]\">View history</a></span></li> </ul>\n</div>\n<div aria-labelledby=\"p-cactions-label\" class=\"vectorMenu emptyPortlet\" id=\"p-cactions\" role=\"navigation\">\n<input aria-labelledby=\"p-cactions-label\" class=\"vectorMenuCheckbox\" type=\"checkbox\"/>\n<h3 id=\"p-cactions-label\"><span>More</span></h3>\n<ul class=\"menu\">\n</ul>\n</div>\n<div id=\"p-search\" role=\"search\">\n<h3>\n<label for=\"searchInput\">Search</label>\n</h3>\n<form action=\"/w/index.php\" id=\"searchform\">\n<div id=\"simpleSearch\">\n<input accesskey=\"f\" id=\"searchInput\" name=\"search\" placeholder=\"Search Wikipedia\" title=\"Search Wikipedia [f]\" type=\"search\"/><input name=\"title\" type=\"hidden\" value=\"Special:Search\"/><input class=\"searchButton mw-fallbackSearchButton\" id=\"mw-searchButton\" name=\"fulltext\" title=\"Search Wikipedia for this text\" type=\"submit\" value=\"Search\"/><input class=\"searchButton\" id=\"searchButton\" name=\"go\" title=\"Go to a page with this exact name if it exists\" type=\"submit\" value=\"Go\"/> </div>\n</form>\n</div>\n</div>\n</div>\n<div id=\"mw-panel\">\n<div id=\"p-logo\" role=\"banner\"><a class=\"mw-wiki-logo\" href=\"/wiki/Main_Page\" title=\"Visit the main page\"></a></div>\n<div aria-labelledby=\"p-navigation-label\" class=\"portal\" id=\"p-navigation\" role=\"navigation\">\n<h3 id=\"p-navigation-label\">Navigation</h3>\n<div class=\"body\">\n<ul>\n<li id=\"n-mainpage-description\"><a accesskey=\"z\" href=\"/wiki/Main_Page\" title=\"Visit the main page [z]\">Main page</a></li><li id=\"n-contents\"><a href=\"/wiki/Portal:Contents\" title=\"Guides to browsing Wikipedia\">Contents</a></li><li id=\"n-featuredcontent\"><a href=\"/wiki/Portal:Featured_content\" title=\"Featured content – the best of Wikipedia\">Featured content</a></li><li id=\"n-currentevents\"><a href=\"/wiki/Portal:Current_events\" title=\"Find background information on current events\">Current events</a></li><li id=\"n-randompage\"><a accesskey=\"x\" href=\"/wiki/Special:Random\" title=\"Load a random article [x]\">Random article</a></li><li id=\"n-sitesupport\"><a href=\"https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&amp;utm_medium=sidebar&amp;utm_campaign=C13_en.wikipedia.org&amp;uselang=en\" title=\"Support us\">Donate to Wikipedia</a></li><li id=\"n-shoplink\"><a href=\"//shop.wikimedia.org\" title=\"Visit the Wikipedia store\">Wikipedia store</a></li> </ul>\n</div>\n</div>\n<div aria-labelledby=\"p-interaction-label\" class=\"portal\" id=\"p-interaction\" role=\"navigation\">\n<h3 id=\"p-interaction-label\">Interaction</h3>\n<div class=\"body\">\n<ul>\n<li id=\"n-help\"><a href=\"/wiki/Help:Contents\" title=\"Guidance on how to use and edit Wikipedia\">Help</a></li><li id=\"n-aboutsite\"><a href=\"/wiki/Wikipedia:About\" title=\"Find out about Wikipedia\">About Wikipedia</a></li><li id=\"n-portal\"><a href=\"/wiki/Wikipedia:Community_portal\" title=\"About the project, what you can do, where to find things\">Community portal</a></li><li id=\"n-recentchanges\"><a accesskey=\"r\" href=\"/wiki/Special:RecentChanges\" title=\"A list of recent changes in the wiki [r]\">Recent changes</a></li><li id=\"n-contactpage\"><a href=\"//en.wikipedia.org/wiki/Wikipedia:Contact_us\" title=\"How to contact Wikipedia\">Contact page</a></li> </ul>\n</div>\n</div>\n<div aria-labelledby=\"p-tb-label\" class=\"portal\" id=\"p-tb\" role=\"navigation\">\n<h3 id=\"p-tb-label\">Tools</h3>\n<div class=\"body\">\n<ul>\n<li id=\"t-whatlinkshere\"><a accesskey=\"j\" href=\"/wiki/Special:WhatLinksHere/Instrumental_convergence\" title=\"List of all English Wikipedia pages containing links to this page [j]\">What links here</a></li><li id=\"t-recentchangeslinked\"><a accesskey=\"k\" href=\"/wiki/Special:RecentChangesLinked/Instrumental_convergence\" rel=\"nofollow\" title=\"Recent changes in pages linked from this page [k]\">Related changes</a></li><li id=\"t-upload\"><a accesskey=\"u\" href=\"/wiki/Wikipedia:File_Upload_Wizard\" title=\"Upload files [u]\">Upload file</a></li><li id=\"t-specialpages\"><a accesskey=\"q\" href=\"/wiki/Special:SpecialPages\" title=\"A list of all special pages [q]\">Special pages</a></li><li id=\"t-permalink\"><a href=\"/w/index.php?title=Instrumental_convergence&amp;oldid=884577623\" title=\"Permanent link to this revision of the page\">Permanent link</a></li><li id=\"t-info\"><a href=\"/w/index.php?title=Instrumental_convergence&amp;action=info\" title=\"More information about this page\">Page information</a></li><li id=\"t-wikibase\"><a accesskey=\"g\" href=\"https://www.wikidata.org/wiki/Special:EntityPage/Q18208100\" title=\"Link to connected data repository item [g]\">Wikidata item</a></li><li id=\"t-cite\"><a href=\"/w/index.php?title=Special:CiteThisPage&amp;page=Instrumental_convergence&amp;id=884577623\" title=\"Information on how to cite this page\">Cite this page</a></li> </ul>\n</div>\n</div>\n<div aria-labelledby=\"p-coll-print_export-label\" class=\"portal\" id=\"p-coll-print_export\" role=\"navigation\">\n<h3 id=\"p-coll-print_export-label\">Print/export</h3>\n<div class=\"body\">\n<ul>\n<li id=\"coll-create_a_book\"><a href=\"/w/index.php?title=Special:Book&amp;bookcmd=book_creator&amp;referer=Instrumental+convergence\">Create a book</a></li><li id=\"coll-download-as-rdf2latex\"><a href=\"/w/index.php?title=Special:ElectronPdf&amp;page=Instrumental+convergence&amp;action=show-download-screen\">Download as PDF</a></li><li id=\"t-print\"><a accesskey=\"p\" href=\"/w/index.php?title=Instrumental_convergence&amp;printable=yes\" title=\"Printable version of this page [p]\">Printable version</a></li> </ul>\n</div>\n</div>\n<div aria-labelledby=\"p-lang-label\" class=\"portal\" id=\"p-lang\" role=\"navigation\">\n<h3 id=\"p-lang-label\">Languages</h3>\n<div class=\"body\">\n<ul>\n</ul>\n<div class=\"after-portlet after-portlet-lang\"><span class=\"wb-langlinks-add wb-langlinks-link\"><a class=\"wbc-editpage\" href=\"https://www.wikidata.org/wiki/Special:EntityPage/Q18208100#sitelinks-wikipedia\" title=\"Add interlanguage links\">Add links</a></span></div> </div>\n</div>\n</div>\n</div>\n<div id=\"footer\" role=\"contentinfo\">\n<ul id=\"footer-info\">\n<li id=\"footer-info-lastmod\"> This page was last edited on 22 February 2019, at 15:24<span class=\"anonymous-show\"> (UTC)</span>.</li>\n<li id=\"footer-info-copyright\">Text is available under the <a href=\"//en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License\" rel=\"license\">Creative Commons Attribution-ShareAlike License</a><a href=\"//creativecommons.org/licenses/by-sa/3.0/\" rel=\"license\" style=\"display:none;\"></a>;\nadditional terms may apply.  By using this site, you agree to the <a href=\"//foundation.wikimedia.org/wiki/Terms_of_Use\">Terms of Use</a> and <a href=\"//foundation.wikimedia.org/wiki/Privacy_policy\">Privacy Policy</a>. Wikipedia® is a registered trademark of the <a href=\"//www.wikimediafoundation.org/\">Wikimedia Foundation, Inc.</a>, a non-profit organization.</li>\n</ul>\n<ul id=\"footer-places\">\n<li id=\"footer-places-privacy\"><a class=\"extiw\" href=\"https://foundation.wikimedia.org/wiki/Privacy_policy\" title=\"wmf:Privacy policy\">Privacy policy</a></li>\n<li id=\"footer-places-about\"><a href=\"/wiki/Wikipedia:About\" title=\"Wikipedia:About\">About Wikipedia</a></li>\n<li id=\"footer-places-disclaimer\"><a href=\"/wiki/Wikipedia:General_disclaimer\" title=\"Wikipedia:General disclaimer\">Disclaimers</a></li>\n<li id=\"footer-places-contact\"><a href=\"//en.wikipedia.org/wiki/Wikipedia:Contact_us\">Contact Wikipedia</a></li>\n<li id=\"footer-places-developers\"><a href=\"https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute\">Developers</a></li>\n<li id=\"footer-places-cookiestatement\"><a href=\"https://foundation.wikimedia.org/wiki/Cookie_statement\">Cookie statement</a></li>\n<li id=\"footer-places-mobileview\"><a class=\"noprint stopMobileRedirectToggle\" href=\"//en.m.wikipedia.org/w/index.php?title=Instrumental_convergence&amp;mobileaction=toggle_view_mobile\">Mobile view</a></li>\n</ul>\n<ul class=\"noprint\" id=\"footer-icons\">\n<li id=\"footer-copyrightico\">\n<a href=\"https://wikimediafoundation.org/\"><img alt=\"Wikimedia Foundation\" height=\"31\" src=\"/static/images/wikimedia-button.png\" srcset=\"/static/images/wikimedia-button-1.5x.png 1.5x, /static/images/wikimedia-button-2x.png 2x\" width=\"88\"/></a> </li>\n<li id=\"footer-poweredbyico\">\n<a href=\"//www.mediawiki.org/\"><img alt=\"Powered by MediaWiki\" height=\"31\" src=\"/static/images/poweredby_mediawiki_88x31.png\" srcset=\"/static/images/poweredby_mediawiki_132x47.png 1.5x, /static/images/poweredby_mediawiki_176x62.png 2x\" width=\"88\"/></a> </li>\n</ul>\n<div style=\"clear: both;\"></div>\n</div>\n<script>(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({\"wgPageParseReport\":{\"limitreport\":{\"cputime\":\"0.344\",\"walltime\":\"0.462\",\"ppvisitednodes\":{\"value\":1584,\"limit\":1000000},\"ppgeneratednodes\":{\"value\":0,\"limit\":1500000},\"postexpandincludesize\":{\"value\":42893,\"limit\":2097152},\"templateargumentsize\":{\"value\":5546,\"limit\":2097152},\"expansiondepth\":{\"value\":12,\"limit\":40},\"expensivefunctioncount\":{\"value\":3,\"limit\":500},\"unstrip-depth\":{\"value\":1,\"limit\":20},\"unstrip-size\":{\"value\":34828,\"limit\":5000000},\"entityaccesscount\":{\"value\":1,\"limit\":400},\"timingprofile\":[\"100.00%  394.629      1 -total\",\" 25.28%   99.779      1 Template:Quote\",\" 23.59%   93.095      1 Template:Reflist\",\" 21.22%   83.751      5 Template:Fix\",\" 20.83%   82.204      4 Template:Cite_news\",\" 20.14%   79.482      4 Template:Fcn\",\" 10.49%   41.391      6 Template:Category_handler\",\" 10.06%   39.718      1 Template:ISBN\",\"  9.21%   36.327      5 Template:Delink\",\"  9.09%   35.865      2 Template:Cite_journal\"]},\"scribunto\":{\"limitreport-timeusage\":{\"value\":\"0.169\",\"limit\":\"10.000\"},\"limitreport-memusage\":{\"value\":4728505,\"limit\":52428800}},\"cachereport\":{\"origin\":\"mw1281\",\"timestamp\":\"20190222152402\",\"ttl\":2073600,\"transientcontent\":false}}});});</script>\n<script type=\"application/ld+json\">{\"@context\":\"https:\\/\\/schema.org\",\"@type\":\"Article\",\"name\":\"Instrumental convergence\",\"url\":\"https:\\/\\/en.wikipedia.org\\/wiki\\/Instrumental_convergence\",\"sameAs\":\"http:\\/\\/www.wikidata.org\\/entity\\/Q18208100\",\"mainEntity\":\"http:\\/\\/www.wikidata.org\\/entity\\/Q18208100\",\"author\":{\"@type\":\"Organization\",\"name\":\"Contributors to Wikimedia projects\"},\"publisher\":{\"@type\":\"Organization\",\"name\":\"Wikimedia Foundation, Inc.\",\"logo\":{\"@type\":\"ImageObject\",\"url\":\"https:\\/\\/www.wikimedia.org\\/static\\/images\\/wmf-hor-googpub.png\"}},\"datePublished\":\"2014-08-18T05:14:31Z\",\"dateModified\":\"2019-02-22T15:24:02Z\"}</script>\n<script>(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({\"wgBackendResponseTime\":108,\"wgHostname\":\"mw1241\"});});</script>\n</body>\n</html>\n",
  "table_of_contents": [
    "1 Instrumental and final goals",
    "2 Hypothetical examples of convergence",
    "2.1 Paperclip maximizer",
    "3 Basic AI drives",
    "3.1 Goal-content integrity",
    "3.1.1 In artificial intelligence",
    "3.2 Resource acquisition",
    "3.3 Cognitive enhancement",
    "3.4 Technological perfection",
    "3.5 Self-preservation",
    "4 Instrumental convergence thesis",
    "5 Impact",
    "6 See also",
    "7 References",
    "8 Notes"
  ],
  "graphics": [],
  "paragraphs": [
    {
      "title": "",
      "text": "Instrumental convergence is the hypothetical tendency for most sufficiently intelligent agents to pursue potentially unbounded instrumental goals such as self-preservation and resource acquisition, provided that their ultimate goals are themselves unbounded.\n\nInstrumental convergence suggests that an intelligent agent with unbounded but apparently harmless goals can act in surprisingly harmful ways. For example, a computer with the sole, unconstrained goal of solving the Riemann hypothesis could attempt to turn the entire Earth into computronium in an effort to increase its computing power so that it can succeed in its calculations.[1]\n\nProposed basic AI drives include utility function or goal-content integrity, self-protection, freedom from interference, self-improvement, and non-satiable acquisition of additional resources.\n\n"
    },
    {
      "title": "Instrumental and final goals",
      "text": "Final goals, or final values, are intrinsically valuable to an intelligent agent, whether an artificial intelligence or a human being, as an end in itself. In contrast, instrumental goals, or instrumental values, are only valuable to an agent as a means toward accomplishing its final goals. The contents and tradeoffs of a completely rational agent's \"final goal\" system can in principle be formalized into a utility function.\n\n"
    },
    {
      "title": "Hypothetical examples of convergence",
      "text": "One hypothetical example of instrumental convergence is provided by the Riemann Hypothesis catastrophe. Marvin Minsky, the co-founder of MIT's AI laboratory, has suggested that an artificial intelligence designed to solve the Riemann hypothesis might decide to take over all of Earth's resources to build supercomputers to help achieve its goal.[1] If the computer had instead been programmed to produce as many paper clips as possible, it would still decide to take all of Earth's resources to meet its final goal.[2] Even though these two final goals are different, both of them produce a convergent instrumental goal of taking over Earth's resources.[3]\n\nThe paperclip maximizer is a thought experiment described by Swedish philosopher Nick Bostrom in 2003. It illustrates the existential risk that an artificial general intelligence may pose to human beings when programmed to pursue even seemingly-harmless goals, and the necessity of incorporating machine ethics into artificial intelligence design. The scenario describes an advanced artificial intelligence tasked with manufacturing paperclips. If such a machine were not programmed to value human life, or to use only designated resources in bounded time, then given enough power its optimized goal would be to turn all matter in the universe, including human beings, into either paperclips or machines which manufacture paperclips.[4]\n\nBostrom has emphasised that he does not believe the paperclip maximiser scenario per se will actually occur; rather, his intention is to illustrate the dangers of creating superintelligent machines without knowing how to safely program them to eliminate existential risk to human beings.[6] The paperclip maximizer example illustrates the broad problem of managing powerful systems that lack human values.[7]\n\n"
    },
    {
      "title": "Basic AI drives",
      "text": "Steve Omohundro has itemized several convergent instrumental goals, including self-preservation or self-protection, utility function or goal-content integrity, self-improvement, and resource acquisition. He refers to these as the \"basic AI drives\". A \"drive\" here denotes a \"tendency which will be present unless specifically counteracted\";[8] this is different from the psychological term \"drive\", denoting an excitatory state produced by a homeostatic disturbance.[9] A tendency for a person to fill out income tax forms every year is a \"drive\" in Omohundro's sense, but not in the psychological sense.[10] Daniel Dewey of the Machine Intelligence Research Institute argues that even an initially introverted self-rewarding AGI may continue to acquire free energy, space, time, and freedom from interference to ensure that it will not be stopped from self-rewarding.[11]\n\nIn humans, maintenance of final goals can be explained with a thought experiment. Suppose a man named \"Gandhi\" has a pill that, if he took it, would cause him to want to kill people. This Gandhi is currently a pacifist: one of his explicit final goals is to never kill anyone. Gandhi is likely to refuse to take the pill, because Gandhi knows that if in the future he wants to kill people, he is likely to actually kill people, and thus the goal of \"not killing people\" would not be satisfied.[12]\n\nHowever, in other cases, people seem happy to let their final values drift. Humans are complicated, and their goals can be inconsistent or unknown, even to themselves.[13]\n\nIn 2009, Jürgen Schmidhuber concluded, in a setting where agents search for proofs about possible self-modifications, \"that any rewrites of the utility function can happen only if the Gödel machine first can prove that the rewrite is useful according to the present utility function.\"[14][15] An analysis by Bill Hibbard of a different scenario is similarly consistent with maintenance of goal content integrity.[15] Hibbard also argues that in a utility maximizing framework the only goal is maximizing expected utility, so that instrumental goals should be called unintended instrumental actions.[16]\n\nMany instrumental goals, such as [...] resource acquisition, are valuable to an agent because they increase its freedom of action.[17][full citation needed]\n\nFor almost any open-ended, non-trivial reward function (or set of goals), possessing more resources (such as equipment, raw materials, or energy) can enable the AI to find a more \"optimal\" solution. Resources can benefit some AIs directly, through being able to create more of whatever stuff its reward function values: \"The AI neither hates you, nor loves you, but you are made out of atoms that it can use for something else.\"[18][19] In addition, almost all AIs can benefit from having more resources to spend on other instrumental goals, such as self-preservation.[19]\n\n\"If the agent's final goals are fairly unbounded and the agent is in a position to become the first superintelligence and thereby obtain a decisive strategic advantage, [...] according to its preferences. At least in this special case, a rational intelligent agent would place a very *high instrumental value on cognitive enhancement*\" [20][page needed]\n\nMany instrumental goals, such as [...] technological advancement, are valuable to an agent because they increase its freedom of action.[17][full citation needed]\n\nMany instrumental goals, such as [...] self-preservation, are valuable to an agent because they increase its freedom of action.[17][full citation needed]\n\n"
    },
    {
      "title": "Instrumental convergence thesis",
      "text": "The instrumental convergence thesis, as outlined by philosopher Nick Bostrom, states:\n\nThe instrumental convergence thesis applies only to instrumental goals; intelligent agents may have a wide variety of possible final goals.[3] Note that by Bostrom's Orthogonality Thesis,[3] final goals of highly intelligent agents may be well-bounded in space, time, and resources; well-bounded ultimate goals do not, in general, engender unbounded instrumental goals.[21]\n\n"
    },
    {
      "title": "Impact",
      "text": "Agents can acquire resources by trade or by conquest. A rational agent will, by definition, choose whatever option will maximize its implicit utility function; therefore a rational agent will trade for a subset of another agent's resources only if outright seizing the resources is too risky or costly (compared with the gains from taking all the resources), or if some other element in its utility function bars it from the seizure. In the case of a powerful, self-interested, rational superintelligence interacting with a lesser intelligence, peaceful trade (rather than unilateral seizure) seems unnecessary and suboptimal, and therefore unlikely.[17][full citation needed]\n\nSome observers, such as Skype's Jaan Tallinn and physicist Max Tegmark, believe that \"basic AI drives\", and other unintended consequences of superintelligent AI programmed by well-meaning programmers, could pose a significant threat to human survival, especially if an \"intelligence explosion\" abruptly occurs due to recursive self-improvement. Since nobody knows how to predict beforehand when superintelligence will arrive, such observers call for research into friendly artificial intelligence as a possible way to mitigate existential risk from artificial general intelligence.[22]\n\n"
    }
  ],
  "links": []
}
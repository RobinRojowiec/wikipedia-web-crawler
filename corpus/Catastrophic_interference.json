{
  "url": "https://en.wikipedia.org/wiki/Catastrophic_interference",
  "title": "Catastrophic interference",
  "html": "<!DOCTYPE html>\n<html class=\"client-nojs\" dir=\"ltr\" lang=\"en\">\n<head>\n<meta charset=\"utf-8\"/>\n<title>Catastrophic interference - Wikipedia</title>\n<script>document.documentElement.className = document.documentElement.className.replace( /(^|\\s)client-nojs(\\s|$)/, \"$1client-js$2\" );</script>\n<script>(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({\"wgCanonicalNamespace\":\"\",\"wgCanonicalSpecialPageName\":false,\"wgNamespaceNumber\":0,\"wgPageName\":\"Catastrophic_interference\",\"wgTitle\":\"Catastrophic interference\",\"wgCurRevisionId\":884965379,\"wgRevisionId\":884965379,\"wgArticleId\":39182554,\"wgIsArticle\":true,\"wgIsRedirect\":false,\"wgAction\":\"view\",\"wgUserName\":null,\"wgUserGroups\":[\"*\"],\"wgCategories\":[\"Articles needing cleanup from April 2013\",\"All pages needing cleanup\",\"Cleanup tagged articles with a reason field from April 2013\",\"Wikipedia pages needing cleanup from April 2013\",\"Wikipedia articles that are too technical from August 2018\",\"All articles that are too technical\",\"Articles needing expert attention from August 2018\",\"All articles needing expert attention\",\"Articles that may contain original research from April 2013\",\"All articles that may contain original research\",\"Articles with multiple maintenance issues\",\"Wikipedia articles needing clarification from July 2016\",\"Artificial neural networks\",\"Artificial intelligence\",\"Machine learning\"],\"wgBreakFrames\":false,\"wgPageContentLanguage\":\"en\",\"wgPageContentModel\":\"wikitext\",\"wgSeparatorTransformTable\":[\"\",\"\"],\"wgDigitTransformTable\":[\"\",\"\"],\"wgDefaultDateFormat\":\"dmy\",\"wgMonthNames\":[\"\",\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"December\"],\"wgMonthNamesShort\":[\"\",\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"],\"wgRelevantPageName\":\"Catastrophic_interference\",\"wgRelevantArticleId\":39182554,\"wgRequestId\":\"XHNhdApAADkAACr3R@cAAADQ\",\"wgCSPNonce\":false,\"wgIsProbablyEditable\":true,\"wgRelevantPageIsProbablyEditable\":true,\"wgRestrictionEdit\":[],\"wgRestrictionMove\":[],\"wgFlaggedRevsParams\":{\"tags\":{}},\"wgStableRevisionId\":null,\"wgCategoryTreePageCategoryOptions\":\"{\\\"mode\\\":0,\\\"hideprefix\\\":20,\\\"showcount\\\":true,\\\"namespaces\\\":false}\",\"wgWikiEditorEnabledModules\":[],\"wgBetaFeaturesFeatures\":[],\"wgMediaViewerOnClick\":true,\"wgMediaViewerEnabledByDefault\":true,\"wgPopupsShouldSendModuleToUser\":true,\"wgPopupsConflictsWithNavPopupGadget\":false,\"wgVisualEditor\":{\"pageLanguageCode\":\"en\",\"pageLanguageDir\":\"ltr\",\"pageVariantFallbacks\":\"en\",\"usePageImages\":true,\"usePageDescriptions\":true},\"wgMFIsPageContentModelEditable\":true,\"wgMFEnableFontChanger\":true,\"wgMFDisplayWikibaseDescriptions\":{\"search\":true,\"nearby\":true,\"watchlist\":true,\"tagline\":false},\"wgRelatedArticles\":null,\"wgRelatedArticlesUseCirrusSearch\":true,\"wgRelatedArticlesOnlyUseCirrusSearch\":false,\"wgWMESchemaEditAttemptStepOversample\":false,\"wgPoweredByHHVM\":true,\"wgULSCurrentAutonym\":\"English\",\"wgNoticeProject\":\"wikipedia\",\"wgCentralNoticeCookiesToDelete\":[],\"wgCentralNoticeCategoriesUsingLegacy\":[\"Fundraising\",\"fundraising\"],\"wgWikibaseItemId\":\"Q16251345\",\"wgScoreNoteLanguages\":{\"arabic\":\"العربية\",\"catalan\":\"català\",\"deutsch\":\"Deutsch\",\"english\":\"English\",\"espanol\":\"español\",\"italiano\":\"italiano\",\"nederlands\":\"Nederlands\",\"norsk\":\"norsk\",\"portugues\":\"português\",\"suomi\":\"suomi\",\"svenska\":\"svenska\",\"vlaams\":\"West-Vlams\"},\"wgScoreDefaultNoteLanguage\":\"nederlands\",\"wgCentralAuthMobileDomain\":false,\"wgCodeMirrorEnabled\":true,\"wgVisualEditorToolbarScrollOffset\":0,\"wgVisualEditorUnsupportedEditParams\":[\"undo\",\"undoafter\",\"veswitched\"],\"wgEditSubmitButtonLabelPublish\":true,\"oresWikiId\":\"enwiki\",\"oresBaseUrl\":\"http://ores.discovery.wmnet:8081/\",\"oresApiVersion\":3});mw.loader.state({\"ext.gadget.charinsert-styles\":\"ready\",\"ext.globalCssJs.user.styles\":\"ready\",\"ext.globalCssJs.site.styles\":\"ready\",\"site.styles\":\"ready\",\"noscript\":\"ready\",\"user.styles\":\"ready\",\"ext.globalCssJs.user\":\"ready\",\"ext.globalCssJs.site\":\"ready\",\"user\":\"ready\",\"user.options\":\"ready\",\"user.tokens\":\"loading\",\"ext.cite.styles\":\"ready\",\"mediawiki.legacy.shared\":\"ready\",\"mediawiki.legacy.commonPrint\":\"ready\",\"jquery.makeCollapsible.styles\":\"ready\",\"mediawiki.toc.styles\":\"ready\",\"wikibase.client.init\":\"ready\",\"ext.visualEditor.desktopArticleTarget.noscript\":\"ready\",\"ext.uls.interlanguage\":\"ready\",\"ext.wikimediaBadges\":\"ready\",\"ext.3d.styles\":\"ready\",\"mediawiki.skinning.interface\":\"ready\",\"skins.vector.styles\":\"ready\"});mw.loader.implement(\"user.tokens@0tffind\",function($,jQuery,require,module){/*@nomin*/mw.user.tokens.set({\"editToken\":\"+\\\\\",\"patrolToken\":\"+\\\\\",\"watchToken\":\"+\\\\\",\"csrfToken\":\"+\\\\\"});\n});RLPAGEMODULES=[\"ext.cite.ux-enhancements\",\"site\",\"mediawiki.page.startup\",\"mediawiki.page.ready\",\"jquery.makeCollapsible\",\"mediawiki.toc\",\"mediawiki.searchSuggest\",\"ext.gadget.teahouse\",\"ext.gadget.ReferenceTooltips\",\"ext.gadget.watchlist-notice\",\"ext.gadget.DRN-wizard\",\"ext.gadget.charinsert\",\"ext.gadget.refToolbar\",\"ext.gadget.extra-toolbar-buttons\",\"ext.gadget.switcher\",\"ext.centralauth.centralautologin\",\"mmv.head\",\"mmv.bootstrap.autostart\",\"ext.popups\",\"ext.visualEditor.desktopArticleTarget.init\",\"ext.visualEditor.targetLoader\",\"ext.eventLogging\",\"ext.wikimediaEvents\",\"ext.navigationTiming\",\"ext.uls.eventlogger\",\"ext.uls.init\",\"ext.uls.compactlinks\",\"ext.uls.interface\",\"ext.quicksurveys.init\",\"ext.centralNotice.geoIP\",\"ext.centralNotice.startUp\",\"skins.vector.js\"];mw.loader.load(RLPAGEMODULES);});</script>\n<link href=\"/w/load.php?debug=false&amp;lang=en&amp;modules=ext.3d.styles%7Cext.cite.styles%7Cext.uls.interlanguage%7Cext.visualEditor.desktopArticleTarget.noscript%7Cext.wikimediaBadges%7Cjquery.makeCollapsible.styles%7Cmediawiki.legacy.commonPrint%2Cshared%7Cmediawiki.skinning.interface%7Cmediawiki.toc.styles%7Cskins.vector.styles%7Cwikibase.client.init&amp;only=styles&amp;skin=vector\" rel=\"stylesheet\"/>\n<script async=\"\" src=\"/w/load.php?debug=false&amp;lang=en&amp;modules=startup&amp;only=scripts&amp;skin=vector\"></script>\n<meta content=\"\" name=\"ResourceLoaderDynamicStyles\"/>\n<link href=\"/w/load.php?debug=false&amp;lang=en&amp;modules=ext.gadget.charinsert-styles&amp;only=styles&amp;skin=vector\" rel=\"stylesheet\"/>\n<link href=\"/w/load.php?debug=false&amp;lang=en&amp;modules=site.styles&amp;only=styles&amp;skin=vector\" rel=\"stylesheet\"/>\n<meta content=\"MediaWiki 1.33.0-wmf.18\" name=\"generator\"/>\n<meta content=\"origin\" name=\"referrer\"/>\n<meta content=\"origin-when-crossorigin\" name=\"referrer\"/>\n<meta content=\"origin-when-cross-origin\" name=\"referrer\"/>\n<link href=\"android-app://org.wikipedia/http/en.m.wikipedia.org/wiki/Catastrophic_interference\" rel=\"alternate\"/>\n<link href=\"/w/index.php?title=Catastrophic_interference&amp;action=edit\" rel=\"alternate\" title=\"Edit this page\" type=\"application/x-wiki\"/>\n<link href=\"/w/index.php?title=Catastrophic_interference&amp;action=edit\" rel=\"edit\" title=\"Edit this page\"/>\n<link href=\"/static/apple-touch/wikipedia.png\" rel=\"apple-touch-icon\"/>\n<link href=\"/static/favicon/wikipedia.ico\" rel=\"shortcut icon\"/>\n<link href=\"/w/opensearch_desc.php\" rel=\"search\" title=\"Wikipedia (en)\" type=\"application/opensearchdescription+xml\"/>\n<link href=\"//en.wikipedia.org/w/api.php?action=rsd\" rel=\"EditURI\" type=\"application/rsd+xml\"/>\n<link href=\"//creativecommons.org/licenses/by-sa/3.0/\" rel=\"license\"/>\n<link href=\"https://en.wikipedia.org/wiki/Catastrophic_interference\" rel=\"canonical\"/>\n<link href=\"//login.wikimedia.org\" rel=\"dns-prefetch\"/>\n<link href=\"//meta.wikimedia.org\" rel=\"dns-prefetch\"/>\n<!--[if lt IE 9]><script src=\"/w/load.php?debug=false&amp;lang=en&amp;modules=html5shiv&amp;only=scripts&amp;skin=vector&amp;sync=1\"></script><![endif]-->\n</head>\n<body class=\"mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject mw-editable page-Catastrophic_interference rootpage-Catastrophic_interference skin-vector action-view\"> <div class=\"noprint\" id=\"mw-page-base\"></div>\n<div class=\"noprint\" id=\"mw-head-base\"></div>\n<div class=\"mw-body\" id=\"content\" role=\"main\">\n<a id=\"top\"></a>\n<div class=\"mw-body-content\" id=\"siteNotice\"><!-- CentralNotice --></div><div class=\"mw-indicators mw-body-content\">\n</div>\n<h1 class=\"firstHeading\" id=\"firstHeading\" lang=\"en\">Catastrophic interference</h1> <div class=\"mw-body-content\" id=\"bodyContent\">\n<div class=\"noprint\" id=\"siteSub\">From Wikipedia, the free encyclopedia</div> <div id=\"contentSub\"></div>\n<div id=\"jump-to-nav\"></div> <a class=\"mw-jump-link\" href=\"#mw-head\">Jump to navigation</a>\n<a class=\"mw-jump-link\" href=\"#p-search\">Jump to search</a>\n<div class=\"mw-content-ltr\" dir=\"ltr\" id=\"mw-content-text\" lang=\"en\"><div class=\"mw-parser-output\"><table class=\"box-Multiple_issues plainlinks metadata ambox ambox-content ambox-multiple_issues compact-ambox\" role=\"presentation\"><tbody><tr><td class=\"mbox-image\"><div style=\"width:52px\"><img alt=\"\" data-file-height=\"40\" data-file-width=\"40\" decoding=\"async\" height=\"40\" src=\"//upload.wikimedia.org/wikipedia/en/thumb/b/b4/Ambox_important.svg/40px-Ambox_important.svg.png\" srcset=\"//upload.wikimedia.org/wikipedia/en/thumb/b/b4/Ambox_important.svg/60px-Ambox_important.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/b/b4/Ambox_important.svg/80px-Ambox_important.svg.png 2x\" width=\"40\"/></div></td><td class=\"mbox-text\"><div class=\"mbox-text-span\"><div class=\"mw-collapsible\" style=\"width:95%; margin: 0.2em 0;\"><b>This article has multiple issues.</b> Please help <b><a class=\"external text\" href=\"//en.wikipedia.org/w/index.php?title=Catastrophic_interference&amp;action=edit\">improve it</a></b> or discuss these issues on the <b><a href=\"/wiki/Talk:Catastrophic_interference\" title=\"Talk:Catastrophic interference\">talk page</a></b>. <small><i>(<a href=\"/wiki/Help:Maintenance_template_removal\" title=\"Help:Maintenance template removal\">Learn how and when to remove these template messages</a>)</i></small>\n<div class=\"mw-collapsible-content\" style=\"margin-top: 0.3em;\">\n<table class=\"box-Cleanup plainlinks metadata ambox ambox-style ambox-Cleanup\" role=\"presentation\"><tbody><tr><td class=\"mbox-image\"><div style=\"width:52px\"><img alt=\"\" data-file-height=\"48\" data-file-width=\"48\" decoding=\"async\" height=\"40\" src=\"//upload.wikimedia.org/wikipedia/en/thumb/f/f2/Edit-clear.svg/40px-Edit-clear.svg.png\" srcset=\"//upload.wikimedia.org/wikipedia/en/thumb/f/f2/Edit-clear.svg/60px-Edit-clear.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/f/f2/Edit-clear.svg/80px-Edit-clear.svg.png 2x\" width=\"40\"/></div></td><td class=\"mbox-text\"><div class=\"mbox-text-span\">This article may <b>require <a href=\"/wiki/Wikipedia:Cleanup\" title=\"Wikipedia:Cleanup\">cleanup</a></b> to meet Wikipedia's <a href=\"/wiki/Wikipedia:Manual_of_Style\" title=\"Wikipedia:Manual of Style\">quality standards</a>. The specific problem is: <b>Some formatting issues, and \"prominent researchers\" should only be wikilinks not external links; could probably also use more wikilinks</b><span class=\"hide-when-compact\"> Please help <a class=\"external text\" href=\"//en.wikipedia.org/w/index.php?title=Catastrophic_interference&amp;action=edit\">improve this article</a> if you can.</span> <small class=\"date-container\"><i>(<span class=\"date\">April 2013</span>)</i></small><small class=\"hide-when-compact\"><i> (<a href=\"/wiki/Help:Maintenance_template_removal\" title=\"Help:Maintenance template removal\">Learn how and when to remove this template message</a>)</i></small></div></td></tr></tbody></table>\n<table class=\"box-Technical plainlinks metadata ambox ambox-style ambox-technical\" role=\"presentation\"><tbody><tr><td class=\"mbox-image\"><div style=\"width:52px\"><img alt=\"\" data-file-height=\"48\" data-file-width=\"48\" decoding=\"async\" height=\"40\" src=\"//upload.wikimedia.org/wikipedia/en/thumb/f/f2/Edit-clear.svg/40px-Edit-clear.svg.png\" srcset=\"//upload.wikimedia.org/wikipedia/en/thumb/f/f2/Edit-clear.svg/60px-Edit-clear.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/f/f2/Edit-clear.svg/80px-Edit-clear.svg.png 2x\" width=\"40\"/></div></td><td class=\"mbox-text\"><div class=\"mbox-text-span\">This article <b>may be too technical for most readers to understand</b>. Please <a class=\"external text\" href=\"//en.wikipedia.org/w/index.php?title=Catastrophic_interference&amp;action=edit\">help improve it</a> to <a href=\"/wiki/Wikipedia:Make_technical_articles_understandable\" title=\"Wikipedia:Make technical articles understandable\">make it understandable to non-experts</a>, without removing the technical details.  <small class=\"date-container\"><i>(<span class=\"date\">August 2018</span>)</i></small><small class=\"hide-when-compact\"><i> (<a href=\"/wiki/Help:Maintenance_template_removal\" title=\"Help:Maintenance template removal\">Learn how and when to remove this template message</a>)</i></small></div></td></tr></tbody></table>\n<table class=\"box-Original_research plainlinks metadata ambox ambox-content ambox-Original_research\" role=\"presentation\"><tbody><tr><td class=\"mbox-image\"><div style=\"width:52px\"><img alt=\"\" data-file-height=\"40\" data-file-width=\"40\" decoding=\"async\" height=\"40\" src=\"//upload.wikimedia.org/wikipedia/en/thumb/b/b4/Ambox_important.svg/40px-Ambox_important.svg.png\" srcset=\"//upload.wikimedia.org/wikipedia/en/thumb/b/b4/Ambox_important.svg/60px-Ambox_important.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/b/b4/Ambox_important.svg/80px-Ambox_important.svg.png 2x\" width=\"40\"/></div></td><td class=\"mbox-text\"><div class=\"mbox-text-span\">This article  <b>possibly contains <a href=\"/wiki/Wikipedia:No_original_research\" title=\"Wikipedia:No original research\">original research</a></b>.<span class=\"hide-when-compact\"> Please <a class=\"external text\" href=\"//en.wikipedia.org/w/index.php?title=Catastrophic_interference&amp;action=edit\">improve it</a> by <a href=\"/wiki/Wikipedia:Verifiability\" title=\"Wikipedia:Verifiability\">verifying</a> the claims made and adding <a href=\"/wiki/Wikipedia:Citing_sources#Inline_citations\" title=\"Wikipedia:Citing sources\">inline citations</a>. Statements consisting only of original research should be removed.</span> <small class=\"date-container\"><i>(<span class=\"date\">April 2013</span>)</i></small><small class=\"hide-when-compact\"><i> (<a href=\"/wiki/Help:Maintenance_template_removal\" title=\"Help:Maintenance template removal\">Learn how and when to remove this template message</a>)</i></small></div></td></tr></tbody></table>\n</div>\n</div><small class=\"hide-when-compact\"><i> (<a href=\"/wiki/Help:Maintenance_template_removal\" title=\"Help:Maintenance template removal\">Learn how and when to remove this template message</a>)</i></small></div></td></tr></tbody></table>\n<p><b>Catastrophic interference</b>, also known as catastrophic forgetting, is the tendency of an <a href=\"/wiki/Artificial_neural_network\" title=\"Artificial neural network\">artificial neural network</a> to completely and abruptly forget previously learned information upon learning new information.<sup class=\"reference\" id=\"cite_ref-McCloskey1989_1-0\"><a href=\"#cite_note-McCloskey1989-1\">[1]</a></sup><sup class=\"reference\" id=\"cite_ref-Ratcliff1990_2-0\"><a href=\"#cite_note-Ratcliff1990-2\">[2]</a></sup> Neural networks are an important part of the <a href=\"/wiki/Connectionism\" title=\"Connectionism\">network approach and connectionist approach</a> to <a href=\"/wiki/Cognitive_science\" title=\"Cognitive science\">cognitive science</a>. These networks use computer simulations to try to model human behaviours, such as memory and learning. Catastrophic interference is an important issue to consider when creating connectionist models of memory. It was originally brought to the attention of the scientific community by research from McCloskey and Cohen (1989),<sup class=\"reference\" id=\"cite_ref-McCloskey1989_1-1\"><a href=\"#cite_note-McCloskey1989-1\">[1]</a></sup> and Ratcliff (1990).<sup class=\"reference\" id=\"cite_ref-Ratcliff1990_2-1\"><a href=\"#cite_note-Ratcliff1990-2\">[2]</a></sup> It is a radical manifestation of the 'sensitivity-stability' dilemma<sup class=\"reference\" id=\"cite_ref-3\"><a href=\"#cite_note-3\">[3]</a></sup> or the 'stability-plasticity' dilemma.<sup class=\"reference\" id=\"cite_ref-4\"><a href=\"#cite_note-4\">[4]</a></sup> Specifically, these problems refer to the issue of being able to make an artificial neural network that is sensitive to, but not disrupted by, new information. <a href=\"/wiki/Lookup_table\" title=\"Lookup table\">Lookup tables</a> and connectionist networks lie on the opposite sides of the stability plasticity spectrum.<sup class=\"reference\" id=\"cite_ref-French1997_5-0\"><a href=\"#cite_note-French1997-5\">[5]</a></sup> The former remains completely stable in the presence of new information but lacks the ability to <a href=\"/wiki/Machine_learning#Generalization\" title=\"Machine learning\">generalize</a>, i.e. infer general principles, from new inputs. On the other hand, connectionist networks like the <a href=\"/wiki/Backpropagation\" title=\"Backpropagation\">standard backpropagation network</a> are very sensitive to new information and can generalize on new inputs. Backpropagation models can be considered good models of <a class=\"mw-redirect\" href=\"/wiki/Human_memory\" title=\"Human memory\">human memory</a> insofar as they mirror the human ability to generalize but these networks often exhibit less stability than human memory. Notably, these backpropagation networks are susceptible to catastrophic interference. This is considered an issue when attempting to model human memory because, unlike these networks, humans typically do not show catastrophic forgetting. Thus, the issue of catastrophic interference must be eradicated from these backpropagation models in order to enhance the plausibility as models of human memory.\n</p>\n<div class=\"toc\" id=\"toc\"><input class=\"toctogglecheckbox\" id=\"toctogglecheckbox\" role=\"button\" style=\"display:none\" type=\"checkbox\"/><div class=\"toctitle\" dir=\"ltr\" lang=\"en\"><h2>Contents</h2><span class=\"toctogglespan\"><label class=\"toctogglelabel\" for=\"toctogglecheckbox\"></label></span></div>\n<ul>\n<li class=\"toclevel-1 tocsection-1\"><a href=\"#History_of_catastrophic_interference\"><span class=\"tocnumber\">1</span> <span class=\"toctext\">History of catastrophic interference</span></a>\n<ul>\n<li class=\"toclevel-2 tocsection-2\"><a href=\"#The_Sequential_Learning_Problem:_McCloskey_and_Cohen_(1989)\"><span class=\"tocnumber\">1.1</span> <span class=\"toctext\"><i>The Sequential Learning Problem</i>: McCloskey and Cohen (1989)</span></a></li>\n<li class=\"toclevel-2 tocsection-3\"><a href=\"#Constraints_Imposed_by_Learning_and_Forgetting_Functions:_Ratcliff_(1990)\"><span class=\"tocnumber\">1.2</span> <span class=\"toctext\"><i>Constraints Imposed by Learning and Forgetting Functions</i>: Ratcliff (1990)</span></a></li>\n</ul>\n</li>\n<li class=\"toclevel-1 tocsection-4\"><a href=\"#Proposed_solutions\"><span class=\"tocnumber\">2</span> <span class=\"toctext\">Proposed solutions</span></a>\n<ul>\n<li class=\"toclevel-2 tocsection-5\"><a href=\"#Node_sharpening_technique\"><span class=\"tocnumber\">2.1</span> <span class=\"toctext\">Node sharpening technique</span></a></li>\n<li class=\"toclevel-2 tocsection-6\"><a href=\"#Novelty_rule\"><span class=\"tocnumber\">2.2</span> <span class=\"toctext\">Novelty rule</span></a></li>\n<li class=\"toclevel-2 tocsection-7\"><a href=\"#Pre-training_networks\"><span class=\"tocnumber\">2.3</span> <span class=\"toctext\">Pre-training networks</span></a></li>\n<li class=\"toclevel-2 tocsection-8\"><a href=\"#Pseudo-recurrent_networks\"><span class=\"tocnumber\">2.4</span> <span class=\"toctext\">Pseudo-recurrent networks</span></a></li>\n<li class=\"toclevel-2 tocsection-9\"><a href=\"#Neural_networks_with_self-refreshing_memory\"><span class=\"tocnumber\">2.5</span> <span class=\"toctext\">Neural networks with self-refreshing memory</span></a></li>\n<li class=\"toclevel-2 tocsection-10\"><a href=\"#Latent_learning\"><span class=\"tocnumber\">2.6</span> <span class=\"toctext\">Latent learning</span></a></li>\n<li class=\"toclevel-2 tocsection-11\"><a href=\"#Elastic_weight_consolidation\"><span class=\"tocnumber\">2.7</span> <span class=\"toctext\">Elastic weight consolidation</span></a></li>\n<li class=\"toclevel-2 tocsection-12\"><a href=\"#Anapoiesis\"><span class=\"tocnumber\">2.8</span> <span class=\"toctext\">Anapoiesis</span></a></li>\n</ul>\n</li>\n<li class=\"toclevel-1 tocsection-13\"><a href=\"#References\"><span class=\"tocnumber\">3</span> <span class=\"toctext\">References</span></a></li>\n</ul>\n</div>\n<h2><span class=\"mw-headline\" id=\"History_of_catastrophic_interference\">History of catastrophic interference</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Catastrophic_interference&amp;action=edit&amp;section=1\" title=\"Edit section: History of catastrophic interference\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p>The term catastrophic interference was originally coined by McCloskey and Cohen (1989) but was also brought to the attention of the scientific community by research from Ratcliff (1990).<sup class=\"reference\" id=\"cite_ref-Ratcliff1990_2-2\"><a href=\"#cite_note-Ratcliff1990-2\">[2]</a></sup>\n</p>\n<h3><span id=\"The_Sequential_Learning_Problem:_McCloskey_and_Cohen_.281989.29\"></span><span class=\"mw-headline\" id=\"The_Sequential_Learning_Problem:_McCloskey_and_Cohen_(1989)\"><i>The Sequential Learning Problem</i>: McCloskey and Cohen (1989)</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Catastrophic_interference&amp;action=edit&amp;section=2\" title=\"Edit section: The Sequential Learning Problem: McCloskey and Cohen (1989)\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>McCloskey and Cohen (1989) noted the problem of catastrophic interference during two different experiments with backpropagation neural network modelling.\n</p>\n<ul><li>Experiment 1: <i>Learning the ones and twos addition facts</i></li></ul>\n<p>In their first experiment they trained a standard backpropagation neural network on a single training set consisting of 17 single-digit ones problems (i.e., 1 + 1 through 9 + 1, and 1 + 2 through 1 + 9) until the network could represent and respond properly to all of them. The error between the actual output and the desired output steadily declined across training sessions, which reflected that the network learned to represent the target outputs better across trials.  Next they trained the network on a single training set consisting of 17 single-digit twos problems (i.e., 2 + 1 through 2 + 9, and 1 + 2 through 9 + 2) until the network could represent, respond properly to all of them. They noted that their procedure was similar to how a child would learn their addition facts. Following each learning trial on the twos facts, the network was tested for its knowledge on both the ones and twos addition facts. Like the ones facts, the twos facts were readily learned by the network. However, McCloskey and Cohen noted the network was no longer able to properly answer the ones addition problems even after one learning trial of the twos addition problems. The output pattern produced in response to the ones facts often resembled an output pattern for an incorrect number more closely than the output pattern for an incorrect number.<sup class=\"noprint Inline-Template\" style=\"margin-left:0.1em; white-space:nowrap;\">[<i><a href=\"/wiki/Wikipedia:Please_clarify\" title=\"Wikipedia:Please clarify\"><span title=\"The text near this tag may need clarification or removal of jargon. (July 2016)\">clarification needed</span></a></i>]</sup> This is considered to be a drastic amount of error. Furthermore, the problems 2+1 and 2+1, which were included in both training sets, even showed dramatic disruption during the first learning trials of the twos facts.\n</p>\n<ul><li>Experiment 2: <i>Replication of Barnes and Underwood (1959) study</i><sup class=\"reference\" id=\"cite_ref-Barnes1959_6-0\"><a href=\"#cite_note-Barnes1959-6\">[6]</a></sup></li></ul>\n<p>In their second connectionist model, McCloskey and Cohen attempted to replicate the study on retroactive interference in humans by Barnes and Underwood (1959). They trained the model on A-B and A-C lists and used a context pattern in the input vector (input pattern), to differentiate between the lists. Specifically the network was trained to responds with the right B response when shown the A stimulus and A-B context pattern and to respond with the correct C response when shown the A stimulus and the A-C context pattern. When the model was trained concurrently on the A-B and A-C items then the network readily learned all of the associations correctly. In sequential training the A-B list was trained first, followed by the A-C list. After each presentation of the A-C list, performance was measured for both the A-B and A-C lists. They found that the amount of training on the A-C list in Barnes and Underwood study that lead to 50% correct responses, lead to nearly 0% correct responses by the backpropagation network. Furthermore, they found that the network tended to show responses that looked like the C response pattern when the network was prompted to give the B response pattern. This indicated that the A-C list apparently had overwritten the A-B list. This could be likened to learning the word dog, followed by learning the word stool and then finding that you cannot recognize the word cat well but instead think of the word stool when presented with the word dog.\n</p><p>McCloskey and Cohen tried to reduce interference through a number of manipulations including changing the number of hidden units, changing the value of the learning rate parameter, overtraining on the A-B list, freezing certain connection weights, changing target values 0 and 1 instead 0.1 and 0.9. However none of these manipulations satisfactorily reduced the catastrophic interference exhibited by the networks.\n</p><p>Overall, McCloskey and Cohen (1989) concluded that: \n</p>\n<ul><li>at least some interference will occur whenever new learning alters the weights involved representing</li>\n<li>the greater the amount of new learning, the greater the disruption in old knowledge</li>\n<li>interference was catastrophic in the backpropagation networks when learning was sequential but not concurrent</li></ul>\n<h3><span id=\"Constraints_Imposed_by_Learning_and_Forgetting_Functions:_Ratcliff_.281990.29\"></span><span class=\"mw-headline\" id=\"Constraints_Imposed_by_Learning_and_Forgetting_Functions:_Ratcliff_(1990)\"><i>Constraints Imposed by Learning and Forgetting Functions</i>: Ratcliff (1990)</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Catastrophic_interference&amp;action=edit&amp;section=3\" title=\"Edit section: Constraints Imposed by Learning and Forgetting Functions: Ratcliff (1990)\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>Ratcliff (1990) used multiple sets of backpropagation models applied to standard recognition memory procedures, in which the items were sequentially learned.<sup class=\"reference\" id=\"cite_ref-Ratcliff1990_2-3\"><a href=\"#cite_note-Ratcliff1990-2\">[2]</a></sup> After inspecting the recognition performance models he found two major problems:\n</p>\n<ul><li>Well-learned information was catastrophically forgotten as new information was learned in both small and large backpropagation networks.</li></ul>\n<p>Even one learning trial with new information resulted in a significant loss of the old information, paralleling the findings of McCloskey and Cohen (1989).<sup class=\"reference\" id=\"cite_ref-McCloskey1989_1-2\"><a href=\"#cite_note-McCloskey1989-1\">[1]</a></sup> Ratcliff also found that the resulting outputs were often a blend of the previous input and the new input. In larger networks, items learned in groups (e.g. AB then CD) were more resistant to forgetting than were items learned singly (e.g. A then B then C…). However, the forgetting for items learned in groups was still large. Adding new hidden units to the network did not reduce interference. \n</p>\n<ul><li>Discrimination between the studied items and previously unseen items decreased as the network learned more.</li></ul>\n<p>This finding contradicts with studies on human memory, which indicated that discrimination increases with learning. Ratcliff attempted to alleviate this problem by adding 'response nodes' that would selectively respond to old and new inputs. However, this method did not work as these response nodes would become active for all inputs. A model which used a context pattern also failed to increase discrimination between new and old items.\n</p>\n<h2><span class=\"mw-headline\" id=\"Proposed_solutions\">Proposed solutions</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Catastrophic_interference&amp;action=edit&amp;section=4\" title=\"Edit section: Proposed solutions\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p>Many researchers have suggested that the main cause of catastrophic interference is overlap in the representations at the hidden layer of distributed neural networks.<sup class=\"reference\" id=\"cite_ref-French1991_7-0\"><a href=\"#cite_note-French1991-7\">[7]</a></sup><sup class=\"reference\" id=\"cite_ref-McRae1993_8-0\"><a href=\"#cite_note-McRae1993-8\">[8]</a></sup><sup class=\"reference\" id=\"cite_ref-French1999_9-0\"><a href=\"#cite_note-French1999-9\">[9]</a></sup> In a distributed representation any given input will tend to create changes in the weights to many of the nodes. Catastrophic forgetting occurs because when many of the weights where \"knowledge is stored\" are changed, it is impossible for prior knowledge to be kept intact. During sequential learning, the inputs become mixed with the new input being superimposed over top of the old input.<sup class=\"reference\" id=\"cite_ref-McRae1993_8-1\"><a href=\"#cite_note-McRae1993-8\">[8]</a></sup> Another way to conceptualize this is through visualizing learning as movement through a weight space.<sup class=\"reference\" id=\"cite_ref-Lewandowsky1991_10-0\"><a href=\"#cite_note-Lewandowsky1991-10\">[10]</a></sup> This weight space can be likened to a spatial representation of all of the possible combinations of weights that the network can possess. When a network first learns to represent a set of patterns, it has found a point in weight space which allows it to recognize all of the patterns that it has seen.<sup class=\"reference\" id=\"cite_ref-French1999_9-1\"><a href=\"#cite_note-French1999-9\">[9]</a></sup> However, when the network learns a new set of patterns sequentially it will move to a place in the weight space that allows it to only recognize the new pattern.<sup class=\"reference\" id=\"cite_ref-French1999_9-2\"><a href=\"#cite_note-French1999-9\">[9]</a></sup> To recognize both sets of patterns, the network must find a place in weight space that can represent both the new and the old output. One way to do this is by connecting a hidden unit to only a subset of the input units. This reduces the likelihood that two different inputs will be encoded by the same hidden units and weights, and so will decrease the chance of interference.<sup class=\"reference\" id=\"cite_ref-McRae1993_8-2\"><a href=\"#cite_note-McRae1993-8\">[8]</a></sup>  Indeed, a number of the proposed solutions to catastrophic interference involve reducing the amount of overlap that occurs when storing information in these weights.\n</p><p>Many of the early techniques in reducing representational overlap involved making either the input vectors or the hidden unit activation patterns <a href=\"/wiki/Orthogonality#Definitions\" title=\"Orthogonality\">orthogonal</a> to one another. Lewandowsky and Li (1995)<sup class=\"reference\" id=\"cite_ref-Lewandowsky1995_11-0\"><a href=\"#cite_note-Lewandowsky1995-11\">[11]</a></sup> noted that the interference between sequentially learned patterns is minimized if the input vectors are orthogonal to each other. Input vectors are said to be orthogonal to each other if the pairwise product of their elements across the two vectors sum to zero. For example, the patterns [0,0,1,0] and [0,1,0,0] are said to be orthogonal because (0×0 + 0×1 + 1×0 + 0×0) = 0. One of the techniques which can create orthogonal representations at the hidden layers involves bipolar feature coding (i.e., coding using -1 and 1 rather than 0 and 1).<sup class=\"reference\" id=\"cite_ref-French1999_9-3\"><a href=\"#cite_note-French1999-9\">[9]</a></sup> Orthogonal patterns tend to produce less interference with each other. However, not all learning problems can be represented using these types of vectors and some studies report that the degree of interference is still problematic with orthogonal vectors.<sup class=\"reference\" id=\"cite_ref-Ratcliff1990_2-4\"><a href=\"#cite_note-Ratcliff1990-2\">[2]</a></sup> Simple techniques such as varying the learning rate parameters in the backpropagation equation were not successful in reducing interference. Varying the number of hidden nodes has also been used to try and reduce interference. However, the findings have been mixed, with some studies finding that more hidden units decrease interference<sup class=\"reference\" id=\"cite_ref-12\"><a href=\"#cite_note-12\">[12]</a></sup> and other studies finding it does not.<sup class=\"reference\" id=\"cite_ref-McCloskey1989_1-3\"><a href=\"#cite_note-McCloskey1989-1\">[1]</a></sup><sup class=\"reference\" id=\"cite_ref-Ratcliff1990_2-5\"><a href=\"#cite_note-Ratcliff1990-2\">[2]</a></sup>\n</p><p>Below are a number of techniques which have empirical support in successfully reducing catastrophic interference in backpropagation neural networks:\n</p>\n<h3><span class=\"mw-headline\" id=\"Node_sharpening_technique\">Node sharpening technique</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Catastrophic_interference&amp;action=edit&amp;section=5\" title=\"Edit section: Node sharpening technique\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>French (1991)<sup class=\"reference\" id=\"cite_ref-French1991_7-1\"><a href=\"#cite_note-French1991-7\">[7]</a></sup>  proposed that catastrophic interference arises in <a href=\"/wiki/Feedforward_neural_network\" title=\"Feedforward neural network\">feedforward</a> backpropagation networks due to the interaction of node activations, or activation overlap, that occur in distributed representations at the hidden layer. Specifically, he defined this activation overlap as the average shared activation over all units in the hidden layer, calculated by summing the lowest activation of the nodes at the hidden layer and averaging this sum. For example, if the activations at the hidden layer from one input are (0.3, 0.1, 0.9, 1.0) and the activations from the next input are (0.0, 0.9, 0.1, 0.9) the activation overlap would be (0.0 + 0.1 + 0.1 + 0.9 ) / 4 = 0.275. When using [binary number|binary] representation of input [row vector|vectors], activation values will be 0 through 1, where 0 indicates no activation overlap and 1 indicates full activation overlap. French noted that <a href=\"/wiki/Neural_network\" title=\"Neural network\">neural networks</a> which employ very localized representations do not show catastrophic interference because of the lack of overlap at the hidden layer. That is to say, each input pattern will create a hidden layer representation that involves the activation of only one node, so differed inputs will have an activation overlap of 0. Thus, he suggested that reducing the value of activation overlap at the hidden layer would reduce catastrophic interference in distributed networks. Specifically he proposed that this could be done through changing the distributed representations at the hidden layer to 'semi-distributed' representations. A 'semi-distributed' representation has fewer hidden nodes that are active, and/or a lower activation value for these nodes, for each representation, which will make the representations of the different inputs overlap less at the hidden layer.  French recommended that this could be done through 'activation sharpening', a technique which slightly increases the activation of a certain number of the most active nodes in the hidden layer, slightly reduces the activation of all the other units and then changes the input-to-hidden layer weights to reflect these activation changes (similar to error backpropagation). Overall the guidelines for the process of 'activation sharpening' are as follows:\n</p>\n<ol><li>Perform a forward activation pass by feeding an input from the input layer to the hidden layer and record the activations at the hidden layer</li>\n<li><i><b>\"Sharpen\" the activation</b></i> of x number of most active nodes by a sharpening factor α:\n<dl><dd><i>A<sub>new</sub></i> = <i>A<sub>old</sub></i> + <i>α</i>(1- <i>A<sub>old</sub></i>) \t\tFor nodes to be sharpened, i.e. more activated</dd>\n<dd><i>A<sub>new</sub></i> = <i>A<sub>old</sub></i> – <i>αA<sub>old</sub></i>\t           For all other nodes</dd>\n<dd>French suggested the number of nodes to be sharpened should be log <i>n</i> nodes, where n is the number of hidden layer nodes</dd></dl></li>\n<li>Use the difference between the old activation (A<sub><i>old</i></sub>) and the sharpened activation (<i>A<sub>new</sub></i>) as an error, backpropagate this error to the input layer, and modify the weights of input-to-output appropriately</li>\n<li>Do a full forward pass with the input through to the output layer</li>\n<li>Backpropagate as usual from the output to the input layer</li>\n<li>Repeat</li></ol>\n<p>In his tests of an 8-8-8 (input-hidden-output) node backpropagation network where one node was sharpened, French found that this sharpening paradigm did result in one node being much more active than the other seven. Moreover, when sharpened, this network took one fourth the time to relearn the initial inputs than a standard backpropagation without node sharpening. Relearning is a measure of memory savings and thus extent of forgetting, where more time to relearn suggests more forgetting (<a class=\"mw-redirect\" href=\"/wiki/Herman_Ebbinghaus\" title=\"Herman Ebbinghaus\">Ebbinghaus</a> savings method). A two-node sharpened network performed even slightly better, however if more than two nodes were sharpened forgetting increased again.\n</p><p>According to French, the sharpened activations interfere less with weights in the network than unsharpened weights and this is due specifically to the way that backpropagation algorithm calculates weight changes. Activations near 0 will change the weights of links less than activations near 1. Consequently, when there are many nodes with low activations (due to sharpening), the weights to and from these nodes will be modified much less than the weights on very active nodes. As a result, when a new input is fed into the network, sharpening will reduce activation overlap by limiting the number of highly active hidden units and will reduce the likelihood of representational overlap by reducing the number of weights that are to be changed.  Thus, node sharpening will decrease the amount of disruption in the old weights, which store prior input patterns, thereby reducing the likelihood of catastrophic forgetting.\n</p>\n<h3><span class=\"mw-headline\" id=\"Novelty_rule\">Novelty rule</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Catastrophic_interference&amp;action=edit&amp;section=6\" title=\"Edit section: Novelty rule\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>Kortge (1990)<sup class=\"reference\" id=\"cite_ref-Kortge1990_13-0\"><a href=\"#cite_note-Kortge1990-13\">[13]</a></sup> proposed a learning rule for training neural networks, called the 'novelty rule', to help alleviate catastrophic interference. As its name suggests, this rule helps the neural network to learn only the components of a new input that differ from an old input. Consequently, the novelty rule changes only the weights that were not previously dedicated to storing information, thereby reducing the overlap in representations at the hidden units. Thus, even when inputs are somewhat similar to another, dissimilar representations can be made at the hidden layer. In order to apply the novelty rule, during learning the input pattern is replaced by a novelty vector that represents the components that differ. The novelty vector for the first layer (input units to hidden units) is determined by taking the target pattern away from the current output of the network (the <a href=\"/wiki/Delta_rule\" title=\"Delta rule\">delta rule</a>). For the second layer (hidden units to output units) the novelty vector is simply the activation of the hidden units that resulted from using the novelty vector as an input through the first layer. Weight changes in the network are computed by using a modified delta rule with the <i><b>novelty vector</b></i> replacing the activation value (sum of the inputs):\n</p>\n<dl><dd>Δ<i>w</i><sub><i>ij</i></sub> = <i>k</i>δ<sub><i>i</i></sub> <i>d</i><sub><i>i</i></sub></dd>\n<dd></dd>\n<dd>Δw<sub><i>ij</i></sub> = weight change between nodes <i>i</i> and <i>j</i></dd>\n<dd><i>k</i> = learning rate</dd>\n<dd>δ<sub><i>i</i></sub> = error signal</dd>\n<dd><i>d</i><sub><i>i</i></sub>= novely vector</dd></dl>\n<p>When the novelty rule is used in a standard backpropagation network there is no, or lessened, forgetting of old items when new items are presented sequentially.<sup class=\"reference\" id=\"cite_ref-Kortge1990_13-1\"><a href=\"#cite_note-Kortge1990-13\">[13]</a></sup> However, this rule can only apply to auto-encoder or auto-associative networks, in which the target response for the output layer is identical to the input pattern. This is because the novelty vector would be meaningless if the desired output was not identical to the input as it would be impossible to calculate how much a new input differed from the old input.\n</p>\n<h3><span class=\"mw-headline\" id=\"Pre-training_networks\">Pre-training networks</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Catastrophic_interference&amp;action=edit&amp;section=7\" title=\"Edit section: Pre-training networks\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>McRae and Hetherington (1993)<sup class=\"reference\" id=\"cite_ref-McRae1993_8-3\"><a href=\"#cite_note-McRae1993-8\">[8]</a></sup> argued that humans, unlike most neural networks, do not take on new learning tasks with a random set of weights. Rather, people tend to bring a wealth of prior knowledge to a task and this helps to avoid the problem of interference. They proposed that when a network is pre-trained on a random sample of data prior to starting a sequential learning task that this prior knowledge will naturally constrain how the new information can be incorporated. This would occur because a random sample of data from a domain which has a high degree of internal structure, such as the English language, training would capture the regularities, or recurring patterns, found within that domain. Since the domain is based on regularities, a newly learned item will tend to be similar to the previously learned information, which will allow the network to incorporate new data with little interference with existing data. Specifically, an input vector which follows the same pattern of regularities as the previously trained data should not cause a drastically different pattern of activation at the hidden layer or drastically alter weights.\n</p><p>To test their hypothesis, McRae and Hetherington (1993) compared the performance of a naïve and pre-trained auto-encoder backpropagation network on three simulations of verbal learning tasks. The pre-trained network was trained using letter based representations of English monosyllabic words or English word pairs.  All three tasks involved the learning of some consonant-vowel-consonant (CVC) strings or CVC pairs (list A), followed by training on a second list of these items (list B). Afterwards, the distributions of the hidden node activations were compared between the naïve and pre-trained network. In all three tasks, the representations of a CVC in the naïve network tended to be spread fairly evenly across all hidden nodes, whereas most hidden nodes were inactive in the pre-trained network. Furthermore, in the pre-trained network the representational overlap between CVCs was reduced compared to the naïve network. The pre-trained network also retained some similarity information as the representational overlap between similar CVCs, like \"JEP\" and \"ZEP\", was greater than for dissimilar CVCs, such as \"JEP\" and \"YUG\". This suggests that the pre-trained network had a better ability to generalize, i.e. notice the patterns, than the naïve network.  Most importantly, this reduction in hidden unit activation and representational overlap resulted in significantly less forgetting in the pre-trained network than the naïve network, essentially eliminating catastrophic interference. Essentially, the pre-training acted to create internal orthogonalization of the activations at the hidden layer, which reduced interference.<sup class=\"reference\" id=\"cite_ref-French1999_9-4\"><a href=\"#cite_note-French1999-9\">[9]</a></sup> Thus, pre-training is a simple way to reduce catastrophic forgetting in standard backpropagation networks.\n</p>\n<h3><span class=\"mw-headline\" id=\"Pseudo-recurrent_networks\">Pseudo-recurrent networks</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Catastrophic_interference&amp;action=edit&amp;section=8\" title=\"Edit section: Pseudo-recurrent networks\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3><p>\nFrench (1997) proposed the idea of a pseudo-recurrent backpropagation network in order to help reduce catastrophic interference (see Figure 2).<sup class=\"reference\" id=\"cite_ref-French1997_5-1\"><a href=\"#cite_note-French1997-5\">[5]</a></sup> In this model the network is separated into two functionally distinct but interacting sub-networks. This model is biologically inspired and is based on research from McClelland, McNaughton, and O'Reilly (1995).<sup class=\"reference\" id=\"cite_ref-McClelland1995_14-0\"><a href=\"#cite_note-McClelland1995-14\">[14]</a></sup> In this research McClelland et al. (1995), suggested that the <a href=\"/wiki/Hippocampus\" title=\"Hippocampus\">hippocampus</a> and <a href=\"/wiki/Neocortex\" title=\"Neocortex\">neocortex</a> act as separable but complementary memory systems. Specifically, the hippocampus <a class=\"mw-redirect\" href=\"/wiki/Short_term_memory\" title=\"Short term memory\">short term memory</a> storage and acts gradually over time to transfer memories into the neocortex for <a class=\"mw-redirect\" href=\"/wiki/Long_term_memory\" title=\"Long term memory\">long term memory</a> storage. They suggest that the information that is stored can be \"brought back\" to the hippocampus during active rehearsal, reminiscence, and sleep and renewed activation is what acts to transfer the information to the neocortex over time. In the pseudo-recurrent network, one of the sub-networks acts as an early processing area, akin to the hippocampus, and functions to learn new input patters. The other sub-network acts as a final-storage area, akin to the neocortex. However, unlike in McClelland et al. (1995) model, the final-storage area sends internally generated representation back to the early processing area. This creates a recurrent network. French proposed that this interleaving of old representations with new representations is the only way to reduce radical forgetting. Since the brain would most likely not have access to the original input patterns, the patterns that would be fed back to the neocortex would be internally generated representations called <i>pseudopatterns</i>.  These pseudopatterns are approximations of previous inputs<sup class=\"reference\" id=\"cite_ref-Robins1995_15-0\"><a href=\"#cite_note-Robins1995-15\">[15]</a></sup> and they can be interleaved with the learning of new inputs. </p><div class=\"thumb tright\"><div class=\"thumbinner\" style=\"width:222px;\"><a class=\"image\" href=\"/wiki/File:Pseudorecurrentnetwork.jpg\"><img alt=\"\" class=\"thumbimage\" data-file-height=\"288\" data-file-width=\"288\" decoding=\"async\" height=\"220\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/e/e6/Pseudorecurrentnetwork.jpg/220px-Pseudorecurrentnetwork.jpg\" srcset=\"//upload.wikimedia.org/wikipedia/commons/e/e6/Pseudorecurrentnetwork.jpg 1.5x\" width=\"220\"/></a> <div class=\"thumbcaption\"><div class=\"magnify\"><a class=\"internal\" href=\"/wiki/File:Pseudorecurrentnetwork.jpg\" title=\"Enlarge\"></a></div>Figure 2: The architecture of a pseudo-recurrent network</div></div></div><p>The use of these pseudopatterns could be biologically plausible as parallels between the consolidation of learning that occurs during sleep and the use of interleaved pseudopatterns. Specifically, they both serve to integrate new information with old information without disruption of the old information.<sup class=\"reference\" id=\"cite_ref-Robins1996_16-0\"><a href=\"#cite_note-Robins1996-16\">[16]</a></sup> When given an input (and a teacher value) is fed into the pseudo-recurrent network would act as follows:\n</p><ul><li>When a pattern is fed from the environment (a real input), the information travels both to the early processing area and the final storage area, however the teacher nodes will inhibit the output from the final storage area</li>\n<li>The new pattern is learned by the early processing area by the standard backpropagation algorithm</li>\n<li>At the same time random input is also fed into the network and causes pseudopatterns to be generated by the final storage area</li>\n<li>Output from the final-storage area, in the form of pseudopatterns, will be used as a teacher for the early-processing area. In this way, the pseudopatterns are interleaved with the 'real inputs' from the environment</li>\n<li>Once the new pattern and the pseudopattern are learned by the early processing area, its weights are copied to the corresponding weights in the final storage area.</li></ul>\n<p>When tested on sequential learning of real world patterns, categorization of edible and poisonous mushrooms, the pseudo-recurrent network was shown less interference than a standard backpropagation network. This improvement was with both memory savings and exact recognition of old patterns. When the activation patterns of the pseudo-recurrent network were investigated, it was shown that this network automatically formed semi-distributed representations. Since these types of representations involve fewer nodes being activated for each pattern, it is likely what helped to reduce interference.\n</p><p>Not only did the pseudo-recurrent model show reduced interference but also it models list-length and list-strength effects seen in humans. The list-length effect means that adding new items to a list harms the memory of earlier items. Like humans, the pseudo recurrent network showed a more gradual forgetting when to be trained list is lengthened. The list-strength effect means that when the strength of recognition for one item is increased, there is no effect on the recognition of the other list items. This is an important finding as other models often exhibit a decrease in the recognition of other list items when one list item is strengthened. Since the direct copying of weights from the early processing area to the final storage area does not seem highly biologically plausible, the transfer of information to the final storage area can be done through training the final storage area with pseudopatterns created by the early processing area. However, a disadvantage of the pseudo-recurrent model is that the number of hidden units in the early processing and final storage sub-networks must be identical.\n</p>\n<h3><span class=\"mw-headline\" id=\"Neural_networks_with_self-refreshing_memory\">Neural networks with self-refreshing memory</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Catastrophic_interference&amp;action=edit&amp;section=9\" title=\"Edit section: Neural networks with self-refreshing memory\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>Following the same basic idea contributed by Robins,<sup class=\"reference\" id=\"cite_ref-Robins1995_15-1\"><a href=\"#cite_note-Robins1995-15\">[15]</a></sup><sup class=\"reference\" id=\"cite_ref-Robins1996_16-1\"><a href=\"#cite_note-Robins1996-16\">[16]</a></sup> Ans and Rousset (1997)<sup class=\"reference\" id=\"cite_ref-17\"><a href=\"#cite_note-17\">[17]</a></sup> have also proposed a two-network artificial neural architecture with <i>memory self-refreshing</i> that overcomes catastrophic interference when sequential learning tasks are carried out in distributed networks trained by backpropagation. The principle is to interleave, at the time when new external patterns are learned, those to-be-learned new external patterns with internally generated pseudopatterns, or 'pseudo-memories', that reflect the previously learned information. What mainly distinguishes this model from those that use classical pseudorehearsal in feedforward multilayer networks is a <i>reverberating</i> process that is used for generating pseudopatterns. This process which, after a number of activity re-injections from a single random seed, tends to go up to nonlinear network <i>attractors</i>, is more suitable for optimally capturing the deep structure of previously learned knowledge than a single feedforward pass of random activation. Ans and Rousset (2000)<sup class=\"reference\" id=\"cite_ref-18\"><a href=\"#cite_note-18\">[18]</a></sup> have shown that the learning mechanism they proposed avoiding catastrophic forgetting, provides a more appropriate way to deal with knowledge transfer as measured by learning speed, ability to generalize and vulnerability to network damages. Musca, Rousset and Ans (2009)<sup class=\"reference\" id=\"cite_ref-19\"><a href=\"#cite_note-19\">[19]</a></sup> have also shown that pseudopatterns originating from an artificial reverberating neural network could induce familiarity in humans with never seen items in the way predicted by simulations conducted with a two-network artificial neural architecture. Furthermore, Ans (2004)<sup class=\"reference\" id=\"cite_ref-20\"><a href=\"#cite_note-20\">[20]</a></sup> has implemented a version of the self-refreshing mechanism using only one network trained by the Contrastive Hebbian Learning rule, a training rule considered as more realistic than the largely used backpropagation algorithm, but fortunately equivalent to the latter.<sup class=\"reference\" id=\"cite_ref-21\"><a href=\"#cite_note-21\">[21]</a></sup>\n<br/>\nSo far, the different solutions to catastrophic interference that have been presented concern tasks of sequential learning involving only non-temporally ordered lists of items. But, to be credible, the self-refreshing mechanism for 'static' learning has to encompass our human ability to learn serially many temporal sequences of patterns without catastrophic interference (e.g. learning one song followed by learning a second song without forgetting the first one). This was done by Ans, Rousset, French and Musca (2004)<sup class=\"reference\" id=\"cite_ref-22\"><a href=\"#cite_note-22\">[22]</a></sup> who have presented, in addition to simulation work, an experiment that evidences a close similarity between the behaviour of humans and the behaviour of the proposed neuromimetic architecture.\n</p>\n<h3><span class=\"mw-headline\" id=\"Latent_learning\">Latent learning</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Catastrophic_interference&amp;action=edit&amp;section=10\" title=\"Edit section: Latent learning\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>Latent Learning is a technique used by Gutstein &amp; Stump (2015)<sup class=\"reference\" id=\"cite_ref-23\"><a href=\"#cite_note-23\">[23]</a></sup> both to mitigate catastrophic interference and to take advantage of transfer learning. Rather than manipulating the representations for new classes used by the hidden nodes, this approach tries to train optimal representations for new classes into the output nodes. It chooses output encodings that are least likely to catastrophically interfere with existing responses.\n</p><p>Given a net that has learned to discriminate among one set of classes using Error Correcting Output Codes (ECOC)<sup class=\"reference\" id=\"cite_ref-24\"><a href=\"#cite_note-24\">[24]</a></sup> (as opposed to <a href=\"/wiki/One-hot\" title=\"One-hot\">1 hot codes</a>), optimal encodings for new classes are chosen by observing the net's average responses to them. Since these average responses arose while learning the original set of classes <i>without any exposure to the new classes</i>, they are referred to as 'Latently Learned Encodings'. This terminology borrows from the concept of <a class=\"new\" href=\"/w/index.php?title=Latent_Learning&amp;action=edit&amp;redlink=1\" title=\"Latent Learning (page does not exist)\">Latent Learning</a>, as introduced by Tolman in 1930.<sup class=\"reference\" id=\"cite_ref-25\"><a href=\"#cite_note-25\">[25]</a></sup> In effect, this technique uses transfer learning to avoid catastrophic interference, by making a net's responses to new classes as consistent as possible with existing responses to classes already learned.\n</p>\n<h3><span class=\"mw-headline\" id=\"Elastic_weight_consolidation\">Elastic weight consolidation</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Catastrophic_interference&amp;action=edit&amp;section=11\" title=\"Edit section: Elastic weight consolidation\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>Kirkpatrick et al. (2017)<sup class=\"reference\" id=\"cite_ref-26\"><a href=\"#cite_note-26\">[26]</a></sup> demonstrated a method to train a single artificial neural network on multiple tasks using a technique called elastic weight consolidation.\n</p>\n<h3><span class=\"mw-headline\" id=\"Anapoiesis\">Anapoiesis</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Catastrophic_interference&amp;action=edit&amp;section=12\" title=\"Edit section: Anapoiesis\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p><a class=\"mw-redirect\" href=\"/wiki/Practopoietic_theory\" title=\"Practopoietic theory\">Practopoietic theory</a><sup class=\"reference\" id=\"cite_ref-Nikolic2014_27-0\"><a href=\"#cite_note-Nikolic2014-27\">[27]</a></sup> proposes that biological systems solve the problem of catastrophic interference by storing long-term memories only in a general form, not applicable to a given situation but instead loosely applicable to a class of different situations. In order to adjust the loosely applicable knowledge to the given current situation, the process of <i>anapoiesis</i> is applied. Anapoiesis stands for \"reconstruction of knowledge\"—transforming knowledge from a general form to a specific one. Practopoietic theory is founded in the <a href=\"/wiki/Variety_(cybernetics)\" title=\"Variety (cybernetics)\">theorems</a> of <a href=\"/wiki/Cybernetics\" title=\"Cybernetics\">cybernetics</a> and is concerned with the question of how cybernetic systems obtain their capabilities to control and act.\n</p>\n<h2><span class=\"mw-headline\" id=\"References\">References</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Catastrophic_interference&amp;action=edit&amp;section=13\" title=\"Edit section: References\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<div class=\"reflist columns references-column-width\" style=\"-moz-column-width: 30em; -webkit-column-width: 30em; column-width: 30em; list-style-type: decimal;\">\n<ol class=\"references\">\n<li id=\"cite_note-McCloskey1989-1\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-McCloskey1989_1-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-McCloskey1989_1-1\"><sup><i><b>b</b></i></sup></a> <a href=\"#cite_ref-McCloskey1989_1-2\"><sup><i><b>c</b></i></sup></a> <a href=\"#cite_ref-McCloskey1989_1-3\"><sup><i><b>d</b></i></sup></a></span> <span class=\"reference-text\">McCloskey, M. &amp; Cohen, N. (1989) <a class=\"external text\" href=\"https://www.sciencedirect.com/science/article/pii/S0079742108605368\" rel=\"nofollow\">Catastrophic interference in connectionist networks: The sequential learning problem</a>. In G. H. Bower (ed.) <i>The Psychology of Learning and Motivation</i>,<i>24</i>, 109-164</span>\n</li>\n<li id=\"cite_note-Ratcliff1990-2\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-Ratcliff1990_2-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-Ratcliff1990_2-1\"><sup><i><b>b</b></i></sup></a> <a href=\"#cite_ref-Ratcliff1990_2-2\"><sup><i><b>c</b></i></sup></a> <a href=\"#cite_ref-Ratcliff1990_2-3\"><sup><i><b>d</b></i></sup></a> <a href=\"#cite_ref-Ratcliff1990_2-4\"><sup><i><b>e</b></i></sup></a> <a href=\"#cite_ref-Ratcliff1990_2-5\"><sup><i><b>f</b></i></sup></a></span> <span class=\"reference-text\">Ratcliff, R. (1990) <a class=\"external text\" href=\"https://pdfs.semanticscholar.org/591b/52d24eb95f5ec3622b814bc91ac872acda9e.pdf\" rel=\"nofollow\">Connectionist models of recognition memory: Constraints imposed by learning and forgetting functions</a>. <i>Psychological Review</i>,<i>97</i>, 285-308</span>\n</li>\n<li id=\"cite_note-3\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-3\">^</a></b></span> <span class=\"reference-text\">Hebb, D.O. (1949). '`Organization of Behaviour'`. New York: Wiley</span>\n</li>\n<li id=\"cite_note-4\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-4\">^</a></b></span> <span class=\"reference-text\">Caroebterm G., &amp; Grossberg, S. (1987) <a class=\"external text\" href=\"https://www.osapublishing.org/viewmedia.cfm?uri=ao-26-23-4919&amp;seq=0\" rel=\"nofollow\">ART 2: Self-organization of stable category recognition codes for analog input patterns</a>. ``Applied Optics, 26``, 4919-4930</span>\n</li>\n<li id=\"cite_note-French1997-5\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-French1997_5-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-French1997_5-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\">French, R. M. (1997) <a class=\"external text\" href=\"http://leadserv.u-bourgogne.fr/files/publications/000287-pseudo-recurrent-connectionist-networks-an-approach-to-the-sensitivity-satbility-dilemma.pdf\" rel=\"nofollow\">Pseudo-recurrent connectionist networks: an approach to the 'sensitivity-stability' dilemma</a>. <i>Connection Science</i>, <i>9</i>(4), 353–379.</span>\n</li>\n<li id=\"cite_note-Barnes1959-6\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-Barnes1959_6-0\">^</a></b></span> <span class=\"reference-text\">Barnes, J. M., &amp; Underwood, B. J. (1959). <a class=\"external text\" href=\"https://psycnet.apa.org/record/1960-03903-001\" rel=\"nofollow\">Fate of first-list associations in transfer theory</a>. <i>Journal of Experimental Psychology</i>, <i>58</i>, 97–105.</span>\n</li>\n<li id=\"cite_note-French1991-7\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-French1991_7-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-French1991_7-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\">French, R. M. (1991). <a class=\"external text\" href=\"http://www.aaai.org/Papers/Symposia/Spring/1993/SS-93-06/SS93-06-007.pdf\" rel=\"nofollow\">Using Semi-Distributed Representations to Overcome Catastrophic Forgetting in Connectioniost Networks</a>. In:  <i>Proceedings of the 13th Annual Cognitive Science Society Conference</i> (pp. 173-178) New Jersey: Lawrence Erlbaum.</span>\n</li>\n<li id=\"cite_note-McRae1993-8\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-McRae1993_8-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-McRae1993_8-1\"><sup><i><b>b</b></i></sup></a> <a href=\"#cite_ref-McRae1993_8-2\"><sup><i><b>c</b></i></sup></a> <a href=\"#cite_ref-McRae1993_8-3\"><sup><i><b>d</b></i></sup></a></span> <span class=\"reference-text\">McRae, K., &amp; Hetherington, P. (1993). <a class=\"external text\" href=\"https://sites.google.com/site/kenmcraelab/publications/McRae_Heth_CogSci_93.pdf\" rel=\"nofollow\">Catastrophic Interference is Eliminated in Pre-Trained Networks</a>. In: <i>Proceedings of the 15th Annual Conference of the Cognitive Science Society</i> (pp. 723-728). Hillsdale, NJ: Lawrence Erlbaum</span>\n</li>\n<li id=\"cite_note-French1999-9\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-French1999_9-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-French1999_9-1\"><sup><i><b>b</b></i></sup></a> <a href=\"#cite_ref-French1999_9-2\"><sup><i><b>c</b></i></sup></a> <a href=\"#cite_ref-French1999_9-3\"><sup><i><b>d</b></i></sup></a> <a href=\"#cite_ref-French1999_9-4\"><sup><i><b>e</b></i></sup></a></span> <span class=\"reference-text\">French, R. M. (1999). <a class=\"external text\" href=\"https://www.researchgate.net/profile/Robert_French/publication/228051810_Catastrophic_Forgetting_in_Connectionist_Networks/links/5a25c2f10f7e9b71dd09cf84/Catastrophic-Forgetting-in-Connectionist-Networks.pdf\" rel=\"nofollow\">Catastrophic forgetting in connectionist networks</a>. <i>Trends in Cognitive Sciences</i>, <i>3</i>(4), 128–135.</span>\n</li>\n<li id=\"cite_note-Lewandowsky1991-10\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-Lewandowsky1991_10-0\">^</a></b></span> <span class=\"reference-text\">Lewandowsky S. (1991). <a class=\"external text\" href=\"https://books.google.com/books?hl=en&amp;lr=&amp;id=LQh7AgAAQBAJ&amp;oi=fnd&amp;pg=PA445&amp;dq=%22Gradual+unlearning+and+catastrophic+interference:+a+comparison+of+distributed+architectures%22&amp;ots=WwiRSSEVfe&amp;sig=OJUPPpmD_VE795zJx88lHUIUxYk#v=onepage&amp;q=%22Gradual%20unlearning%20and%20catastrophic%20interference%3A%20a%20comparison%20of%20distributed%20architectures%22&amp;f=false\" rel=\"nofollow\">Gradual unlearning and catastrophic interference: a comparison of distributed architectures</a>. In: Hockley WE and Lewandowsky S (eds). <i>Relating theory and data: essays on human memory in honor of Bennet B. Murdock</i> (pp. 445–476). Hillsdale, NJ: Lawrence Erlbaum</span>\n</li>\n<li id=\"cite_note-Lewandowsky1995-11\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-Lewandowsky1995_11-0\">^</a></b></span> <span class=\"reference-text\">Lewandowsky, S., &amp; Li, S-C. (1995). <a class=\"external text\" href=\"https://www.sciencedirect.com/science/article/pii/B9780122089305500118\" rel=\"nofollow\">Catastrophic interference in neural networks: causes, solutions, and data</a>. In: Dempster, F.N. &amp; Brainerd, C. (eds). <i>Interference and Inhibition in Cognition</i> (pp. 329–361). San Diego: Academic Press</span>\n</li>\n<li id=\"cite_note-12\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-12\">^</a></b></span> <span class=\"reference-text\">Yamaguchi, M. (2004). <a class=\"external text\" href=\"https://journals.lww.com/neuroreport/Abstract/2004/10250/Reassessment_of_catastrophic_interference.24.aspx\" rel=\"nofollow\">Reassessment of Catastrophic Interference</a>. <i>Computational Neuroscience</i>, <i>15</i>(15), 2423 – 2426</span>\n</li>\n<li id=\"cite_note-Kortge1990-13\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-Kortge1990_13-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-Kortge1990_13-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\">Kortge, C. A. (1990). Episodic memory in connectionist networks. In: <i>The Twelfth Annual Conference of the Cognitive Science Society</i>, (pp. 764-771). Hillsdale, NJ: Lawrence Erlbaum.</span>\n</li>\n<li id=\"cite_note-McClelland1995-14\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-McClelland1995_14-0\">^</a></b></span> <span class=\"reference-text\">McClelland, J., McNaughton, B. &amp; O'Reilly, R. (1995) <a class=\"external text\" href=\"https://pdfs.semanticscholar.org/57f1/16f3e6780424463cc8416ce755a72f873aa9.pdf\" rel=\"nofollow\">Why there are complementary learning systems in the hippocampus and neocortex: Insights from the successes and failures of connectionist models of learning and memory</a>. <i>Psychological Review</i>, <i>102</i>, 419-457.</span>\n</li>\n<li id=\"cite_note-Robins1995-15\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-Robins1995_15-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-Robins1995_15-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\">Robins, A. (1995). <a class=\"external text\" href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.108.3078&amp;rep=rep1&amp;type=pdf\" rel=\"nofollow\">Catastrophic Forgetting, rehearsal and pseudorehearsal</a>. <i>Connection Science</i>, <i>7</i>, 123-146.</span>\n</li>\n<li id=\"cite_note-Robins1996-16\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-Robins1996_16-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-Robins1996_16-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\">Robins, A. (1996). Consolidation in Neural Networks and in the Sleeping Brain. <i>Connection Science</i>, <i>8</i>(2), 259-276.</span>\n</li>\n<li id=\"cite_note-17\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-17\">^</a></b></span> <span class=\"reference-text\">Ans, B., &amp; Rousset, S. (1997). <a class=\"external text\" href=\"https://www.sciencedirect.com/science/article/pii/S0764446997824729\" rel=\"nofollow\">Avoiding catastrophic forgetting by coupling two reverberating neural networks</a>. <i>CR Academie Science Paris, Life Sciences</i>, <i>320</i>, 89-997.</span>\n</li>\n<li id=\"cite_note-18\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-18\">^</a></b></span> <span class=\"reference-text\">Ans, B., &amp; Rousset, S. (2000). Neural networks with a self-refreshing memory: Knowledge transfer in sequential Learning tasks without catastrophic forgetting. <i>Connection Science</i>, <i>12</i>, 1-19.</span>\n</li>\n<li id=\"cite_note-19\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-19\">^</a></b></span> <span class=\"reference-text\">Musca, S. C., Rousset, S., &amp; Ans, B. (2009). <a class=\"external text\" href=\"http://imss-www.upmf-grenoble.fr/prevert/SpecialiteSC/FichiersPDF/MuscaRoussetAns2009.pdf\" rel=\"nofollow\">Artificial neural network whispering to the brain: Nonlinear system attractors induce familiarity with never seen items</a>. <i>Connection Science</i>, <i>21</i>(4), 359-377.</span>\n</li>\n<li id=\"cite_note-20\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-20\">^</a></b></span> <span class=\"reference-text\">Ans, B. (2004). <a class=\"external text\" href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.98.2552&amp;rep=rep1&amp;type=pdf\" rel=\"nofollow\">Sequential learning in distributed neural networks without catastrophic forgetting: A single and realistic self-refreshing memory can do it</a>. <i>Neural Information Processing-Letters and Reviews</i>, <i>4</i>, 27-32.</span>\n</li>\n<li id=\"cite_note-21\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-21\">^</a></b></span> <span class=\"reference-text\">Xie, X.,  &amp; Seung, H. S.  (2003). <a class=\"external text\" href=\"http://www.ics.uci.edu/~xhx/publications/chl_nc.pdf\" rel=\"nofollow\">Equivalence of backpropagation and Contrastive Hebbian Learning in a layered network</a>. <i>Neural Computation</i>, <i>15</i>, 441-454.</span>\n</li>\n<li id=\"cite_note-22\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-22\">^</a></b></span> <span class=\"reference-text\">Ans, B., Rousset, S., French, R. M., &amp; Musca, S. C. (2004). <a class=\"external text\" href=\"https://www.researchgate.net/profile/Robert_French/publication/221704256_Self-refreshing_memory_in_artificial_neural_networks_learning_temporal_sequences_without_catastrophic_forgetting/links/0deec532bcda563147000000/Self-refreshing-memory-in-artificial-neural-networks-learning-temporal-sequences-without-catastrophic-forgetting.pdf\" rel=\"nofollow\">Self-refreshing memory in artificial neural networks: Learning temporal sequences without catastrophic forgetting</a>. <i>Connection Science</i>, <i>16</i>, 71-99.</span>\n</li>\n<li id=\"cite_note-23\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-23\">^</a></b></span> <span class=\"reference-text\">Gutstein and Stump (2015). <a class=\"external text\" href=\"https://ieeexplore.ieee.org/abstract/document/7280416/\" rel=\"nofollow\">Reduction Of Catastrophic Forgetting With Transfer Learning And Ternary Output Codes</a>. In: Proceedings <i>2015 International Joint Conference on Neural Nets</i> (pp 1-8)</span>\n</li>\n<li id=\"cite_note-24\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-24\">^</a></b></span> <span class=\"reference-text\">Dietterich, T. G., &amp; Bakiri, G. (1995). <a class=\"external text\" href=\"https://www.jair.org/index.php/jair/article/download/10127/23985\" rel=\"nofollow\">Solving multiclass learning problems via error-correcting output codes</a>. <i>Journal of Artificial Intelligence Research</i>, (pp. 263-286)</span>\n</li>\n<li id=\"cite_note-25\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-25\">^</a></b></span> <span class=\"reference-text\">Tolman, E.C.; C.H. Honzik (1930). \"\"Insight\" in Rats\". University of California Publications in Psychology.</span>\n</li>\n<li id=\"cite_note-26\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-26\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation arxiv\">Kirkpatrick, James (2016). \"Elastic Weight Consolidation\". <a href=\"/wiki/ArXiv\" title=\"ArXiv\">arXiv</a>:<span class=\"cs1-lock-free\" title=\"Freely accessible\"><a class=\"external text\" href=\"//arxiv.org/abs/1612.00796\" rel=\"nofollow\">1612.00796</a></span> [<a class=\"external text\" href=\"//arxiv.org/archive/cs.LG\" rel=\"nofollow\">cs.LG</a>].</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Elastic+Weight+Consolidation&amp;rft.date=2016&amp;rft_id=info%3Aarxiv%2F1612.00796&amp;rft.aulast=Kirkpatrick&amp;rft.aufirst=James&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ACatastrophic+interference\"></span><style data-mw-deduplicate=\"TemplateStyles:r879151008\">.mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:\"\\\"\"\"\\\"\"\"'\"\"'\"}.mw-parser-output .citation .cs1-lock-free a{background:url(\"//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png\")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url(\"//upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Lock-gray-alt-2.svg/9px-Lock-gray-alt-2.svg.png\")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-subscription a{background:url(\"//upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Lock-red-alt-2.svg/9px-Lock-red-alt-2.svg.png\")no-repeat;background-position:right .1em center}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background:url(\"//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/12px-Wikisource-logo.svg.png\")no-repeat;background-position:right .1em center}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:inherit;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}</style></span>\n</li>\n<li id=\"cite_note-Nikolic2014-27\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-Nikolic2014_27-0\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation web\">Danko Nikolić (2014). <a class=\"external text\" href=\"http://www.danko-nikolic.com/?smd_process_download=1&amp;download_id=724\" rel=\"nofollow\">\"Practopoiesis: Or how life fosters a mind. arXiv:1402.5332 [q-bio.NC]\"</a><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">2014-06-06</span></span>.</cite><span class=\"Z3988\" title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Practopoiesis%3A+Or+how+life+fosters+a+mind.+arXiv%3A1402.5332+%5Bq-bio.NC%5D.&amp;rft.date=2014&amp;rft.au=Danko+Nikoli%C4%87&amp;rft_id=http%3A%2F%2Fwww.danko-nikolic.com%2F%3Fsmd_process_download%3D1%26download_id%3D724&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ACatastrophic+interference\"></span><link href=\"mw-data:TemplateStyles:r879151008\" rel=\"mw-deduplicated-inline-style\"/></span>\n</li>\n</ol></div>\n<!-- \nNewPP limit report\nParsed by mw1316\nCached time: 20190225032910\nCache expiry: 2073600\nDynamic content: false\nCPU time usage: 0.296 seconds\nReal time usage: 0.378 seconds\nPreprocessor visited node count: 1215/1000000\nPreprocessor generated node count: 0/1500000\nPost‐expand include size: 49057/2097152 bytes\nTemplate argument size: 12818/2097152 bytes\nHighest expansion depth: 20/40\nExpensive parser function count: 9/500\nUnstrip recursion depth: 1/20\nUnstrip post‐expand size: 23732/5000000 bytes\nNumber of Wikibase entities loaded: 1/400\nLua time usage: 0.129/10.000 seconds\nLua memory usage: 2.58 MB/50 MB\n-->\n<!--\nTransclusion expansion time report (%,ms,calls,template)\n100.00%  311.878      1 -total\n 44.89%  140.013      1 Template:Reflist\n 41.17%  128.387      4 Template:Ambox\n 37.79%  117.854      1 Template:Multiple_issues\n 33.91%  105.745      1 Template:Cite_arxiv\n 18.57%   57.931      1 Template:Cleanup\n  9.99%   31.146      3 Template:Category_handler\n  8.80%   27.450      3 Template:Main_other\n  8.28%   25.812      1 Template:Clarify\n  7.30%   22.765      1 Template:Fix-span\n-->\n<!-- Saved in parser cache with key enwiki:pcache:idhash:39182554-0!canonical and timestamp 20190225032912 and revision id 884965379\n -->\n</div><noscript><img alt=\"\" height=\"1\" src=\"//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1\" style=\"border: none; position: absolute;\" title=\"\" width=\"1\"/></noscript></div> <div class=\"printfooter\">\n\t\t\t\t\t\tRetrieved from \"<a dir=\"ltr\" href=\"https://en.wikipedia.org/w/index.php?title=Catastrophic_interference&amp;oldid=884965379\">https://en.wikipedia.org/w/index.php?title=Catastrophic_interference&amp;oldid=884965379</a>\"\t\t\t\t\t</div>\n<div class=\"catlinks\" data-mw=\"interface\" id=\"catlinks\"><div class=\"mw-normal-catlinks\" id=\"mw-normal-catlinks\"><a href=\"/wiki/Help:Category\" title=\"Help:Category\">Categories</a>: <ul><li><a href=\"/wiki/Category:Artificial_neural_networks\" title=\"Category:Artificial neural networks\">Artificial neural networks</a></li><li><a href=\"/wiki/Category:Artificial_intelligence\" title=\"Category:Artificial intelligence\">Artificial intelligence</a></li><li><a href=\"/wiki/Category:Machine_learning\" title=\"Category:Machine learning\">Machine learning</a></li></ul></div><div class=\"mw-hidden-catlinks mw-hidden-cats-hidden\" id=\"mw-hidden-catlinks\">Hidden categories: <ul><li><a href=\"/wiki/Category:Articles_needing_cleanup_from_April_2013\" title=\"Category:Articles needing cleanup from April 2013\">Articles needing cleanup from April 2013</a></li><li><a href=\"/wiki/Category:All_pages_needing_cleanup\" title=\"Category:All pages needing cleanup\">All pages needing cleanup</a></li><li><a href=\"/wiki/Category:Cleanup_tagged_articles_with_a_reason_field_from_April_2013\" title=\"Category:Cleanup tagged articles with a reason field from April 2013\">Cleanup tagged articles with a reason field from April 2013</a></li><li><a href=\"/wiki/Category:Wikipedia_pages_needing_cleanup_from_April_2013\" title=\"Category:Wikipedia pages needing cleanup from April 2013\">Wikipedia pages needing cleanup from April 2013</a></li><li><a href=\"/wiki/Category:Wikipedia_articles_that_are_too_technical_from_August_2018\" title=\"Category:Wikipedia articles that are too technical from August 2018\">Wikipedia articles that are too technical from August 2018</a></li><li><a href=\"/wiki/Category:All_articles_that_are_too_technical\" title=\"Category:All articles that are too technical\">All articles that are too technical</a></li><li><a href=\"/wiki/Category:Articles_needing_expert_attention_from_August_2018\" title=\"Category:Articles needing expert attention from August 2018\">Articles needing expert attention from August 2018</a></li><li><a href=\"/wiki/Category:All_articles_needing_expert_attention\" title=\"Category:All articles needing expert attention\">All articles needing expert attention</a></li><li><a href=\"/wiki/Category:Articles_that_may_contain_original_research_from_April_2013\" title=\"Category:Articles that may contain original research from April 2013\">Articles that may contain original research from April 2013</a></li><li><a href=\"/wiki/Category:All_articles_that_may_contain_original_research\" title=\"Category:All articles that may contain original research\">All articles that may contain original research</a></li><li><a href=\"/wiki/Category:Articles_with_multiple_maintenance_issues\" title=\"Category:Articles with multiple maintenance issues\">Articles with multiple maintenance issues</a></li><li><a href=\"/wiki/Category:Wikipedia_articles_needing_clarification_from_July_2016\" title=\"Category:Wikipedia articles needing clarification from July 2016\">Wikipedia articles needing clarification from July 2016</a></li></ul></div></div> <div class=\"visualClear\"></div>\n</div>\n</div>\n<div id=\"mw-navigation\">\n<h2>Navigation menu</h2>\n<div id=\"mw-head\">\n<div aria-labelledby=\"p-personal-label\" id=\"p-personal\" role=\"navigation\">\n<h3 id=\"p-personal-label\">Personal tools</h3>\n<ul>\n<li id=\"pt-anonuserpage\">Not logged in</li><li id=\"pt-anontalk\"><a accesskey=\"n\" href=\"/wiki/Special:MyTalk\" title=\"Discussion about edits from this IP address [n]\">Talk</a></li><li id=\"pt-anoncontribs\"><a accesskey=\"y\" href=\"/wiki/Special:MyContributions\" title=\"A list of edits made from this IP address [y]\">Contributions</a></li><li id=\"pt-createaccount\"><a href=\"/w/index.php?title=Special:CreateAccount&amp;returnto=Catastrophic+interference\" title=\"You are encouraged to create an account and log in; however, it is not mandatory\">Create account</a></li><li id=\"pt-login\"><a accesskey=\"o\" href=\"/w/index.php?title=Special:UserLogin&amp;returnto=Catastrophic+interference\" title=\"You're encouraged to log in; however, it's not mandatory. [o]\">Log in</a></li> </ul>\n</div>\n<div id=\"left-navigation\">\n<div aria-labelledby=\"p-namespaces-label\" class=\"vectorTabs\" id=\"p-namespaces\" role=\"navigation\">\n<h3 id=\"p-namespaces-label\">Namespaces</h3>\n<ul>\n<li class=\"selected\" id=\"ca-nstab-main\"><span><a accesskey=\"c\" href=\"/wiki/Catastrophic_interference\" title=\"View the content page [c]\">Article</a></span></li><li id=\"ca-talk\"><span><a accesskey=\"t\" href=\"/wiki/Talk:Catastrophic_interference\" rel=\"discussion\" title=\"Discussion about the content page [t]\">Talk</a></span></li> </ul>\n</div>\n<div aria-labelledby=\"p-variants-label\" class=\"vectorMenu emptyPortlet\" id=\"p-variants\" role=\"navigation\">\n<input aria-labelledby=\"p-variants-label\" class=\"vectorMenuCheckbox\" type=\"checkbox\"/>\n<h3 id=\"p-variants-label\">\n<span>Variants</span>\n</h3>\n<ul class=\"menu\">\n</ul>\n</div>\n</div>\n<div id=\"right-navigation\">\n<div aria-labelledby=\"p-views-label\" class=\"vectorTabs\" id=\"p-views\" role=\"navigation\">\n<h3 id=\"p-views-label\">Views</h3>\n<ul>\n<li class=\"collapsible selected\" id=\"ca-view\"><span><a href=\"/wiki/Catastrophic_interference\">Read</a></span></li><li class=\"collapsible\" id=\"ca-edit\"><span><a accesskey=\"e\" href=\"/w/index.php?title=Catastrophic_interference&amp;action=edit\" title=\"Edit this page [e]\">Edit</a></span></li><li class=\"collapsible\" id=\"ca-history\"><span><a accesskey=\"h\" href=\"/w/index.php?title=Catastrophic_interference&amp;action=history\" title=\"Past revisions of this page [h]\">View history</a></span></li> </ul>\n</div>\n<div aria-labelledby=\"p-cactions-label\" class=\"vectorMenu emptyPortlet\" id=\"p-cactions\" role=\"navigation\">\n<input aria-labelledby=\"p-cactions-label\" class=\"vectorMenuCheckbox\" type=\"checkbox\"/>\n<h3 id=\"p-cactions-label\"><span>More</span></h3>\n<ul class=\"menu\">\n</ul>\n</div>\n<div id=\"p-search\" role=\"search\">\n<h3>\n<label for=\"searchInput\">Search</label>\n</h3>\n<form action=\"/w/index.php\" id=\"searchform\">\n<div id=\"simpleSearch\">\n<input accesskey=\"f\" id=\"searchInput\" name=\"search\" placeholder=\"Search Wikipedia\" title=\"Search Wikipedia [f]\" type=\"search\"/><input name=\"title\" type=\"hidden\" value=\"Special:Search\"/><input class=\"searchButton mw-fallbackSearchButton\" id=\"mw-searchButton\" name=\"fulltext\" title=\"Search Wikipedia for this text\" type=\"submit\" value=\"Search\"/><input class=\"searchButton\" id=\"searchButton\" name=\"go\" title=\"Go to a page with this exact name if it exists\" type=\"submit\" value=\"Go\"/> </div>\n</form>\n</div>\n</div>\n</div>\n<div id=\"mw-panel\">\n<div id=\"p-logo\" role=\"banner\"><a class=\"mw-wiki-logo\" href=\"/wiki/Main_Page\" title=\"Visit the main page\"></a></div>\n<div aria-labelledby=\"p-navigation-label\" class=\"portal\" id=\"p-navigation\" role=\"navigation\">\n<h3 id=\"p-navigation-label\">Navigation</h3>\n<div class=\"body\">\n<ul>\n<li id=\"n-mainpage-description\"><a accesskey=\"z\" href=\"/wiki/Main_Page\" title=\"Visit the main page [z]\">Main page</a></li><li id=\"n-contents\"><a href=\"/wiki/Portal:Contents\" title=\"Guides to browsing Wikipedia\">Contents</a></li><li id=\"n-featuredcontent\"><a href=\"/wiki/Portal:Featured_content\" title=\"Featured content – the best of Wikipedia\">Featured content</a></li><li id=\"n-currentevents\"><a href=\"/wiki/Portal:Current_events\" title=\"Find background information on current events\">Current events</a></li><li id=\"n-randompage\"><a accesskey=\"x\" href=\"/wiki/Special:Random\" title=\"Load a random article [x]\">Random article</a></li><li id=\"n-sitesupport\"><a href=\"https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&amp;utm_medium=sidebar&amp;utm_campaign=C13_en.wikipedia.org&amp;uselang=en\" title=\"Support us\">Donate to Wikipedia</a></li><li id=\"n-shoplink\"><a href=\"//shop.wikimedia.org\" title=\"Visit the Wikipedia store\">Wikipedia store</a></li> </ul>\n</div>\n</div>\n<div aria-labelledby=\"p-interaction-label\" class=\"portal\" id=\"p-interaction\" role=\"navigation\">\n<h3 id=\"p-interaction-label\">Interaction</h3>\n<div class=\"body\">\n<ul>\n<li id=\"n-help\"><a href=\"/wiki/Help:Contents\" title=\"Guidance on how to use and edit Wikipedia\">Help</a></li><li id=\"n-aboutsite\"><a href=\"/wiki/Wikipedia:About\" title=\"Find out about Wikipedia\">About Wikipedia</a></li><li id=\"n-portal\"><a href=\"/wiki/Wikipedia:Community_portal\" title=\"About the project, what you can do, where to find things\">Community portal</a></li><li id=\"n-recentchanges\"><a accesskey=\"r\" href=\"/wiki/Special:RecentChanges\" title=\"A list of recent changes in the wiki [r]\">Recent changes</a></li><li id=\"n-contactpage\"><a href=\"//en.wikipedia.org/wiki/Wikipedia:Contact_us\" title=\"How to contact Wikipedia\">Contact page</a></li> </ul>\n</div>\n</div>\n<div aria-labelledby=\"p-tb-label\" class=\"portal\" id=\"p-tb\" role=\"navigation\">\n<h3 id=\"p-tb-label\">Tools</h3>\n<div class=\"body\">\n<ul>\n<li id=\"t-whatlinkshere\"><a accesskey=\"j\" href=\"/wiki/Special:WhatLinksHere/Catastrophic_interference\" title=\"List of all English Wikipedia pages containing links to this page [j]\">What links here</a></li><li id=\"t-recentchangeslinked\"><a accesskey=\"k\" href=\"/wiki/Special:RecentChangesLinked/Catastrophic_interference\" rel=\"nofollow\" title=\"Recent changes in pages linked from this page [k]\">Related changes</a></li><li id=\"t-upload\"><a accesskey=\"u\" href=\"/wiki/Wikipedia:File_Upload_Wizard\" title=\"Upload files [u]\">Upload file</a></li><li id=\"t-specialpages\"><a accesskey=\"q\" href=\"/wiki/Special:SpecialPages\" title=\"A list of all special pages [q]\">Special pages</a></li><li id=\"t-permalink\"><a href=\"/w/index.php?title=Catastrophic_interference&amp;oldid=884965379\" title=\"Permanent link to this revision of the page\">Permanent link</a></li><li id=\"t-info\"><a href=\"/w/index.php?title=Catastrophic_interference&amp;action=info\" title=\"More information about this page\">Page information</a></li><li id=\"t-wikibase\"><a accesskey=\"g\" href=\"https://www.wikidata.org/wiki/Special:EntityPage/Q16251345\" title=\"Link to connected data repository item [g]\">Wikidata item</a></li><li id=\"t-cite\"><a href=\"/w/index.php?title=Special:CiteThisPage&amp;page=Catastrophic_interference&amp;id=884965379\" title=\"Information on how to cite this page\">Cite this page</a></li> </ul>\n</div>\n</div>\n<div aria-labelledby=\"p-coll-print_export-label\" class=\"portal\" id=\"p-coll-print_export\" role=\"navigation\">\n<h3 id=\"p-coll-print_export-label\">Print/export</h3>\n<div class=\"body\">\n<ul>\n<li id=\"coll-create_a_book\"><a href=\"/w/index.php?title=Special:Book&amp;bookcmd=book_creator&amp;referer=Catastrophic+interference\">Create a book</a></li><li id=\"coll-download-as-rdf2latex\"><a href=\"/w/index.php?title=Special:ElectronPdf&amp;page=Catastrophic+interference&amp;action=show-download-screen\">Download as PDF</a></li><li id=\"t-print\"><a accesskey=\"p\" href=\"/w/index.php?title=Catastrophic_interference&amp;printable=yes\" title=\"Printable version of this page [p]\">Printable version</a></li> </ul>\n</div>\n</div>\n<div aria-labelledby=\"p-lang-label\" class=\"portal\" id=\"p-lang\" role=\"navigation\">\n<h3 id=\"p-lang-label\">Languages</h3>\n<div class=\"body\">\n<ul>\n</ul>\n<div class=\"after-portlet after-portlet-lang\"><span class=\"wb-langlinks-add wb-langlinks-link\"><a class=\"wbc-editpage\" href=\"https://www.wikidata.org/wiki/Special:EntityPage/Q16251345#sitelinks-wikipedia\" title=\"Add interlanguage links\">Add links</a></span></div> </div>\n</div>\n</div>\n</div>\n<div id=\"footer\" role=\"contentinfo\">\n<ul id=\"footer-info\">\n<li id=\"footer-info-lastmod\"> This page was last edited on 25 February 2019, at 03:29<span class=\"anonymous-show\"> (UTC)</span>.</li>\n<li id=\"footer-info-copyright\">Text is available under the <a href=\"//en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License\" rel=\"license\">Creative Commons Attribution-ShareAlike License</a><a href=\"//creativecommons.org/licenses/by-sa/3.0/\" rel=\"license\" style=\"display:none;\"></a>;\nadditional terms may apply.  By using this site, you agree to the <a href=\"//foundation.wikimedia.org/wiki/Terms_of_Use\">Terms of Use</a> and <a href=\"//foundation.wikimedia.org/wiki/Privacy_policy\">Privacy Policy</a>. Wikipedia® is a registered trademark of the <a href=\"//www.wikimediafoundation.org/\">Wikimedia Foundation, Inc.</a>, a non-profit organization.</li>\n</ul>\n<ul id=\"footer-places\">\n<li id=\"footer-places-privacy\"><a class=\"extiw\" href=\"https://foundation.wikimedia.org/wiki/Privacy_policy\" title=\"wmf:Privacy policy\">Privacy policy</a></li>\n<li id=\"footer-places-about\"><a href=\"/wiki/Wikipedia:About\" title=\"Wikipedia:About\">About Wikipedia</a></li>\n<li id=\"footer-places-disclaimer\"><a href=\"/wiki/Wikipedia:General_disclaimer\" title=\"Wikipedia:General disclaimer\">Disclaimers</a></li>\n<li id=\"footer-places-contact\"><a href=\"//en.wikipedia.org/wiki/Wikipedia:Contact_us\">Contact Wikipedia</a></li>\n<li id=\"footer-places-developers\"><a href=\"https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute\">Developers</a></li>\n<li id=\"footer-places-cookiestatement\"><a href=\"https://foundation.wikimedia.org/wiki/Cookie_statement\">Cookie statement</a></li>\n<li id=\"footer-places-mobileview\"><a class=\"noprint stopMobileRedirectToggle\" href=\"//en.m.wikipedia.org/w/index.php?title=Catastrophic_interference&amp;mobileaction=toggle_view_mobile\">Mobile view</a></li>\n</ul>\n<ul class=\"noprint\" id=\"footer-icons\">\n<li id=\"footer-copyrightico\">\n<a href=\"https://wikimediafoundation.org/\"><img alt=\"Wikimedia Foundation\" height=\"31\" src=\"/static/images/wikimedia-button.png\" srcset=\"/static/images/wikimedia-button-1.5x.png 1.5x, /static/images/wikimedia-button-2x.png 2x\" width=\"88\"/></a> </li>\n<li id=\"footer-poweredbyico\">\n<a href=\"//www.mediawiki.org/\"><img alt=\"Powered by MediaWiki\" height=\"31\" src=\"/static/images/poweredby_mediawiki_88x31.png\" srcset=\"/static/images/poweredby_mediawiki_132x47.png 1.5x, /static/images/poweredby_mediawiki_176x62.png 2x\" width=\"88\"/></a> </li>\n</ul>\n<div style=\"clear: both;\"></div>\n</div>\n<script>(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({\"wgPageParseReport\":{\"limitreport\":{\"cputime\":\"0.296\",\"walltime\":\"0.378\",\"ppvisitednodes\":{\"value\":1215,\"limit\":1000000},\"ppgeneratednodes\":{\"value\":0,\"limit\":1500000},\"postexpandincludesize\":{\"value\":49057,\"limit\":2097152},\"templateargumentsize\":{\"value\":12818,\"limit\":2097152},\"expansiondepth\":{\"value\":20,\"limit\":40},\"expensivefunctioncount\":{\"value\":9,\"limit\":500},\"unstrip-depth\":{\"value\":1,\"limit\":20},\"unstrip-size\":{\"value\":23732,\"limit\":5000000},\"entityaccesscount\":{\"value\":1,\"limit\":400},\"timingprofile\":[\"100.00%  311.878      1 -total\",\" 44.89%  140.013      1 Template:Reflist\",\" 41.17%  128.387      4 Template:Ambox\",\" 37.79%  117.854      1 Template:Multiple_issues\",\" 33.91%  105.745      1 Template:Cite_arxiv\",\" 18.57%   57.931      1 Template:Cleanup\",\"  9.99%   31.146      3 Template:Category_handler\",\"  8.80%   27.450      3 Template:Main_other\",\"  8.28%   25.812      1 Template:Clarify\",\"  7.30%   22.765      1 Template:Fix-span\"]},\"scribunto\":{\"limitreport-timeusage\":{\"value\":\"0.129\",\"limit\":\"10.000\"},\"limitreport-memusage\":{\"value\":2700105,\"limit\":52428800}},\"cachereport\":{\"origin\":\"mw1316\",\"timestamp\":\"20190225032910\",\"ttl\":2073600,\"transientcontent\":false}}});mw.config.set({\"wgBackendResponseTime\":120,\"wgHostname\":\"mw1262\"});});</script>\n</body>\n</html>\n",
  "table_of_contents": [
    "1 History of catastrophic interference",
    "1.1 The Sequential Learning Problem: McCloskey and Cohen (1989)",
    "1.2 Constraints Imposed by Learning and Forgetting Functions: Ratcliff (1990)",
    "2 Proposed solutions",
    "2.1 Node sharpening technique",
    "2.2 Novelty rule",
    "2.3 Pre-training networks",
    "2.4 Pseudo-recurrent networks",
    "2.5 Neural networks with self-refreshing memory",
    "2.6 Latent learning",
    "2.7 Elastic weight consolidation",
    "2.8 Anapoiesis",
    "3 References"
  ],
  "graphics": [
    {
      "url": "/wiki/File:Pseudorecurrentnetwork.jpg",
      "caption": "Figure 2: The architecture of a pseudo-recurrent network"
    }
  ],
  "paragraphs": [
    {
      "title": "",
      "text": "Catastrophic interference, also known as catastrophic forgetting, is the tendency of an artificial neural network to completely and abruptly forget previously learned information upon learning new information.[1][2] Neural networks are an important part of the network approach and connectionist approach to cognitive science. These networks use computer simulations to try to model human behaviours, such as memory and learning. Catastrophic interference is an important issue to consider when creating connectionist models of memory. It was originally brought to the attention of the scientific community by research from McCloskey and Cohen (1989),[1] and Ratcliff (1990).[2] It is a radical manifestation of the 'sensitivity-stability' dilemma[3] or the 'stability-plasticity' dilemma.[4] Specifically, these problems refer to the issue of being able to make an artificial neural network that is sensitive to, but not disrupted by, new information. Lookup tables and connectionist networks lie on the opposite sides of the stability plasticity spectrum.[5] The former remains completely stable in the presence of new information but lacks the ability to generalize, i.e. infer general principles, from new inputs. On the other hand, connectionist networks like the standard backpropagation network are very sensitive to new information and can generalize on new inputs. Backpropagation models can be considered good models of human memory insofar as they mirror the human ability to generalize but these networks often exhibit less stability than human memory. Notably, these backpropagation networks are susceptible to catastrophic interference. This is considered an issue when attempting to model human memory because, unlike these networks, humans typically do not show catastrophic forgetting. Thus, the issue of catastrophic interference must be eradicated from these backpropagation models in order to enhance the plausibility as models of human memory.\n\n"
    },
    {
      "title": "History of catastrophic interference",
      "text": "The term catastrophic interference was originally coined by McCloskey and Cohen (1989) but was also brought to the attention of the scientific community by research from Ratcliff (1990).[2]\n\nMcCloskey and Cohen (1989) noted the problem of catastrophic interference during two different experiments with backpropagation neural network modelling.\n\nIn their first experiment they trained a standard backpropagation neural network on a single training set consisting of 17 single-digit ones problems (i.e., 1 + 1 through 9 + 1, and 1 + 2 through 1 + 9) until the network could represent and respond properly to all of them. The error between the actual output and the desired output steadily declined across training sessions, which reflected that the network learned to represent the target outputs better across trials.  Next they trained the network on a single training set consisting of 17 single-digit twos problems (i.e., 2 + 1 through 2 + 9, and 1 + 2 through 9 + 2) until the network could represent, respond properly to all of them. They noted that their procedure was similar to how a child would learn their addition facts. Following each learning trial on the twos facts, the network was tested for its knowledge on both the ones and twos addition facts. Like the ones facts, the twos facts were readily learned by the network. However, McCloskey and Cohen noted the network was no longer able to properly answer the ones addition problems even after one learning trial of the twos addition problems. The output pattern produced in response to the ones facts often resembled an output pattern for an incorrect number more closely than the output pattern for an incorrect number.[clarification needed] This is considered to be a drastic amount of error. Furthermore, the problems 2+1 and 2+1, which were included in both training sets, even showed dramatic disruption during the first learning trials of the twos facts.\n\nIn their second connectionist model, McCloskey and Cohen attempted to replicate the study on retroactive interference in humans by Barnes and Underwood (1959). They trained the model on A-B and A-C lists and used a context pattern in the input vector (input pattern), to differentiate between the lists. Specifically the network was trained to responds with the right B response when shown the A stimulus and A-B context pattern and to respond with the correct C response when shown the A stimulus and the A-C context pattern. When the model was trained concurrently on the A-B and A-C items then the network readily learned all of the associations correctly. In sequential training the A-B list was trained first, followed by the A-C list. After each presentation of the A-C list, performance was measured for both the A-B and A-C lists. They found that the amount of training on the A-C list in Barnes and Underwood study that lead to 50% correct responses, lead to nearly 0% correct responses by the backpropagation network. Furthermore, they found that the network tended to show responses that looked like the C response pattern when the network was prompted to give the B response pattern. This indicated that the A-C list apparently had overwritten the A-B list. This could be likened to learning the word dog, followed by learning the word stool and then finding that you cannot recognize the word cat well but instead think of the word stool when presented with the word dog.\n\nMcCloskey and Cohen tried to reduce interference through a number of manipulations including changing the number of hidden units, changing the value of the learning rate parameter, overtraining on the A-B list, freezing certain connection weights, changing target values 0 and 1 instead 0.1 and 0.9. However none of these manipulations satisfactorily reduced the catastrophic interference exhibited by the networks.\n\nOverall, McCloskey and Cohen (1989) concluded that: \n\nRatcliff (1990) used multiple sets of backpropagation models applied to standard recognition memory procedures, in which the items were sequentially learned.[2] After inspecting the recognition performance models he found two major problems:\n\nEven one learning trial with new information resulted in a significant loss of the old information, paralleling the findings of McCloskey and Cohen (1989).[1] Ratcliff also found that the resulting outputs were often a blend of the previous input and the new input. In larger networks, items learned in groups (e.g. AB then CD) were more resistant to forgetting than were items learned singly (e.g. A then B then C…). However, the forgetting for items learned in groups was still large. Adding new hidden units to the network did not reduce interference. \n\nThis finding contradicts with studies on human memory, which indicated that discrimination increases with learning. Ratcliff attempted to alleviate this problem by adding 'response nodes' that would selectively respond to old and new inputs. However, this method did not work as these response nodes would become active for all inputs. A model which used a context pattern also failed to increase discrimination between new and old items.\n\n"
    },
    {
      "title": "Proposed solutions",
      "text": "Many researchers have suggested that the main cause of catastrophic interference is overlap in the representations at the hidden layer of distributed neural networks.[7][8][9] In a distributed representation any given input will tend to create changes in the weights to many of the nodes. Catastrophic forgetting occurs because when many of the weights where \"knowledge is stored\" are changed, it is impossible for prior knowledge to be kept intact. During sequential learning, the inputs become mixed with the new input being superimposed over top of the old input.[8] Another way to conceptualize this is through visualizing learning as movement through a weight space.[10] This weight space can be likened to a spatial representation of all of the possible combinations of weights that the network can possess. When a network first learns to represent a set of patterns, it has found a point in weight space which allows it to recognize all of the patterns that it has seen.[9] However, when the network learns a new set of patterns sequentially it will move to a place in the weight space that allows it to only recognize the new pattern.[9] To recognize both sets of patterns, the network must find a place in weight space that can represent both the new and the old output. One way to do this is by connecting a hidden unit to only a subset of the input units. This reduces the likelihood that two different inputs will be encoded by the same hidden units and weights, and so will decrease the chance of interference.[8]  Indeed, a number of the proposed solutions to catastrophic interference involve reducing the amount of overlap that occurs when storing information in these weights.\n\nMany of the early techniques in reducing representational overlap involved making either the input vectors or the hidden unit activation patterns orthogonal to one another. Lewandowsky and Li (1995)[11] noted that the interference between sequentially learned patterns is minimized if the input vectors are orthogonal to each other. Input vectors are said to be orthogonal to each other if the pairwise product of their elements across the two vectors sum to zero. For example, the patterns [0,0,1,0] and [0,1,0,0] are said to be orthogonal because (0×0 + 0×1 + 1×0 + 0×0) = 0. One of the techniques which can create orthogonal representations at the hidden layers involves bipolar feature coding (i.e., coding using -1 and 1 rather than 0 and 1).[9] Orthogonal patterns tend to produce less interference with each other. However, not all learning problems can be represented using these types of vectors and some studies report that the degree of interference is still problematic with orthogonal vectors.[2] Simple techniques such as varying the learning rate parameters in the backpropagation equation were not successful in reducing interference. Varying the number of hidden nodes has also been used to try and reduce interference. However, the findings have been mixed, with some studies finding that more hidden units decrease interference[12] and other studies finding it does not.[1][2]\n\nBelow are a number of techniques which have empirical support in successfully reducing catastrophic interference in backpropagation neural networks:\n\nFrench (1991)[7]  proposed that catastrophic interference arises in feedforward backpropagation networks due to the interaction of node activations, or activation overlap, that occur in distributed representations at the hidden layer. Specifically, he defined this activation overlap as the average shared activation over all units in the hidden layer, calculated by summing the lowest activation of the nodes at the hidden layer and averaging this sum. For example, if the activations at the hidden layer from one input are (0.3, 0.1, 0.9, 1.0) and the activations from the next input are (0.0, 0.9, 0.1, 0.9) the activation overlap would be (0.0 + 0.1 + 0.1 + 0.9 ) / 4 = 0.275. When using [binary number|binary] representation of input [row vector|vectors], activation values will be 0 through 1, where 0 indicates no activation overlap and 1 indicates full activation overlap. French noted that neural networks which employ very localized representations do not show catastrophic interference because of the lack of overlap at the hidden layer. That is to say, each input pattern will create a hidden layer representation that involves the activation of only one node, so differed inputs will have an activation overlap of 0. Thus, he suggested that reducing the value of activation overlap at the hidden layer would reduce catastrophic interference in distributed networks. Specifically he proposed that this could be done through changing the distributed representations at the hidden layer to 'semi-distributed' representations. A 'semi-distributed' representation has fewer hidden nodes that are active, and/or a lower activation value for these nodes, for each representation, which will make the representations of the different inputs overlap less at the hidden layer.  French recommended that this could be done through 'activation sharpening', a technique which slightly increases the activation of a certain number of the most active nodes in the hidden layer, slightly reduces the activation of all the other units and then changes the input-to-hidden layer weights to reflect these activation changes (similar to error backpropagation). Overall the guidelines for the process of 'activation sharpening' are as follows:\n\nIn his tests of an 8-8-8 (input-hidden-output) node backpropagation network where one node was sharpened, French found that this sharpening paradigm did result in one node being much more active than the other seven. Moreover, when sharpened, this network took one fourth the time to relearn the initial inputs than a standard backpropagation without node sharpening. Relearning is a measure of memory savings and thus extent of forgetting, where more time to relearn suggests more forgetting (Ebbinghaus savings method). A two-node sharpened network performed even slightly better, however if more than two nodes were sharpened forgetting increased again.\n\nAccording to French, the sharpened activations interfere less with weights in the network than unsharpened weights and this is due specifically to the way that backpropagation algorithm calculates weight changes. Activations near 0 will change the weights of links less than activations near 1. Consequently, when there are many nodes with low activations (due to sharpening), the weights to and from these nodes will be modified much less than the weights on very active nodes. As a result, when a new input is fed into the network, sharpening will reduce activation overlap by limiting the number of highly active hidden units and will reduce the likelihood of representational overlap by reducing the number of weights that are to be changed.  Thus, node sharpening will decrease the amount of disruption in the old weights, which store prior input patterns, thereby reducing the likelihood of catastrophic forgetting.\n\nKortge (1990)[13] proposed a learning rule for training neural networks, called the 'novelty rule', to help alleviate catastrophic interference. As its name suggests, this rule helps the neural network to learn only the components of a new input that differ from an old input. Consequently, the novelty rule changes only the weights that were not previously dedicated to storing information, thereby reducing the overlap in representations at the hidden units. Thus, even when inputs are somewhat similar to another, dissimilar representations can be made at the hidden layer. In order to apply the novelty rule, during learning the input pattern is replaced by a novelty vector that represents the components that differ. The novelty vector for the first layer (input units to hidden units) is determined by taking the target pattern away from the current output of the network (the delta rule). For the second layer (hidden units to output units) the novelty vector is simply the activation of the hidden units that resulted from using the novelty vector as an input through the first layer. Weight changes in the network are computed by using a modified delta rule with the novelty vector replacing the activation value (sum of the inputs):\n\nWhen the novelty rule is used in a standard backpropagation network there is no, or lessened, forgetting of old items when new items are presented sequentially.[13] However, this rule can only apply to auto-encoder or auto-associative networks, in which the target response for the output layer is identical to the input pattern. This is because the novelty vector would be meaningless if the desired output was not identical to the input as it would be impossible to calculate how much a new input differed from the old input.\n\nMcRae and Hetherington (1993)[8] argued that humans, unlike most neural networks, do not take on new learning tasks with a random set of weights. Rather, people tend to bring a wealth of prior knowledge to a task and this helps to avoid the problem of interference. They proposed that when a network is pre-trained on a random sample of data prior to starting a sequential learning task that this prior knowledge will naturally constrain how the new information can be incorporated. This would occur because a random sample of data from a domain which has a high degree of internal structure, such as the English language, training would capture the regularities, or recurring patterns, found within that domain. Since the domain is based on regularities, a newly learned item will tend to be similar to the previously learned information, which will allow the network to incorporate new data with little interference with existing data. Specifically, an input vector which follows the same pattern of regularities as the previously trained data should not cause a drastically different pattern of activation at the hidden layer or drastically alter weights.\n\nTo test their hypothesis, McRae and Hetherington (1993) compared the performance of a naïve and pre-trained auto-encoder backpropagation network on three simulations of verbal learning tasks. The pre-trained network was trained using letter based representations of English monosyllabic words or English word pairs.  All three tasks involved the learning of some consonant-vowel-consonant (CVC) strings or CVC pairs (list A), followed by training on a second list of these items (list B). Afterwards, the distributions of the hidden node activations were compared between the naïve and pre-trained network. In all three tasks, the representations of a CVC in the naïve network tended to be spread fairly evenly across all hidden nodes, whereas most hidden nodes were inactive in the pre-trained network. Furthermore, in the pre-trained network the representational overlap between CVCs was reduced compared to the naïve network. The pre-trained network also retained some similarity information as the representational overlap between similar CVCs, like \"JEP\" and \"ZEP\", was greater than for dissimilar CVCs, such as \"JEP\" and \"YUG\". This suggests that the pre-trained network had a better ability to generalize, i.e. notice the patterns, than the naïve network.  Most importantly, this reduction in hidden unit activation and representational overlap resulted in significantly less forgetting in the pre-trained network than the naïve network, essentially eliminating catastrophic interference. Essentially, the pre-training acted to create internal orthogonalization of the activations at the hidden layer, which reduced interference.[9] Thus, pre-training is a simple way to reduce catastrophic forgetting in standard backpropagation networks.\n\n\nFrench (1997) proposed the idea of a pseudo-recurrent backpropagation network in order to help reduce catastrophic interference (see Figure 2).[5] In this model the network is separated into two functionally distinct but interacting sub-networks. This model is biologically inspired and is based on research from McClelland, McNaughton, and O'Reilly (1995).[14] In this research McClelland et al. (1995), suggested that the hippocampus and neocortex act as separable but complementary memory systems. Specifically, the hippocampus short term memory storage and acts gradually over time to transfer memories into the neocortex for long term memory storage. They suggest that the information that is stored can be \"brought back\" to the hippocampus during active rehearsal, reminiscence, and sleep and renewed activation is what acts to transfer the information to the neocortex over time. In the pseudo-recurrent network, one of the sub-networks acts as an early processing area, akin to the hippocampus, and functions to learn new input patters. The other sub-network acts as a final-storage area, akin to the neocortex. However, unlike in McClelland et al. (1995) model, the final-storage area sends internally generated representation back to the early processing area. This creates a recurrent network. French proposed that this interleaving of old representations with new representations is the only way to reduce radical forgetting. Since the brain would most likely not have access to the original input patterns, the patterns that would be fed back to the neocortex would be internally generated representations called pseudopatterns.  These pseudopatterns are approximations of previous inputs[15] and they can be interleaved with the learning of new inputs. \nThe use of these pseudopatterns could be biologically plausible as parallels between the consolidation of learning that occurs during sleep and the use of interleaved pseudopatterns. Specifically, they both serve to integrate new information with old information without disruption of the old information.[16] When given an input (and a teacher value) is fed into the pseudo-recurrent network would act as follows:\n\nWhen tested on sequential learning of real world patterns, categorization of edible and poisonous mushrooms, the pseudo-recurrent network was shown less interference than a standard backpropagation network. This improvement was with both memory savings and exact recognition of old patterns. When the activation patterns of the pseudo-recurrent network were investigated, it was shown that this network automatically formed semi-distributed representations. Since these types of representations involve fewer nodes being activated for each pattern, it is likely what helped to reduce interference.\n\nNot only did the pseudo-recurrent model show reduced interference but also it models list-length and list-strength effects seen in humans. The list-length effect means that adding new items to a list harms the memory of earlier items. Like humans, the pseudo recurrent network showed a more gradual forgetting when to be trained list is lengthened. The list-strength effect means that when the strength of recognition for one item is increased, there is no effect on the recognition of the other list items. This is an important finding as other models often exhibit a decrease in the recognition of other list items when one list item is strengthened. Since the direct copying of weights from the early processing area to the final storage area does not seem highly biologically plausible, the transfer of information to the final storage area can be done through training the final storage area with pseudopatterns created by the early processing area. However, a disadvantage of the pseudo-recurrent model is that the number of hidden units in the early processing and final storage sub-networks must be identical.\n\nFollowing the same basic idea contributed by Robins,[15][16] Ans and Rousset (1997)[17] have also proposed a two-network artificial neural architecture with memory self-refreshing that overcomes catastrophic interference when sequential learning tasks are carried out in distributed networks trained by backpropagation. The principle is to interleave, at the time when new external patterns are learned, those to-be-learned new external patterns with internally generated pseudopatterns, or 'pseudo-memories', that reflect the previously learned information. What mainly distinguishes this model from those that use classical pseudorehearsal in feedforward multilayer networks is a reverberating process that is used for generating pseudopatterns. This process which, after a number of activity re-injections from a single random seed, tends to go up to nonlinear network attractors, is more suitable for optimally capturing the deep structure of previously learned knowledge than a single feedforward pass of random activation. Ans and Rousset (2000)[18] have shown that the learning mechanism they proposed avoiding catastrophic forgetting, provides a more appropriate way to deal with knowledge transfer as measured by learning speed, ability to generalize and vulnerability to network damages. Musca, Rousset and Ans (2009)[19] have also shown that pseudopatterns originating from an artificial reverberating neural network could induce familiarity in humans with never seen items in the way predicted by simulations conducted with a two-network artificial neural architecture. Furthermore, Ans (2004)[20] has implemented a version of the self-refreshing mechanism using only one network trained by the Contrastive Hebbian Learning rule, a training rule considered as more realistic than the largely used backpropagation algorithm, but fortunately equivalent to the latter.[21]\n\nSo far, the different solutions to catastrophic interference that have been presented concern tasks of sequential learning involving only non-temporally ordered lists of items. But, to be credible, the self-refreshing mechanism for 'static' learning has to encompass our human ability to learn serially many temporal sequences of patterns without catastrophic interference (e.g. learning one song followed by learning a second song without forgetting the first one). This was done by Ans, Rousset, French and Musca (2004)[22] who have presented, in addition to simulation work, an experiment that evidences a close similarity between the behaviour of humans and the behaviour of the proposed neuromimetic architecture.\n\nLatent Learning is a technique used by Gutstein & Stump (2015)[23] both to mitigate catastrophic interference and to take advantage of transfer learning. Rather than manipulating the representations for new classes used by the hidden nodes, this approach tries to train optimal representations for new classes into the output nodes. It chooses output encodings that are least likely to catastrophically interfere with existing responses.\n\nGiven a net that has learned to discriminate among one set of classes using Error Correcting Output Codes (ECOC)[24] (as opposed to 1 hot codes), optimal encodings for new classes are chosen by observing the net's average responses to them. Since these average responses arose while learning the original set of classes without any exposure to the new classes, they are referred to as 'Latently Learned Encodings'. This terminology borrows from the concept of Latent Learning, as introduced by Tolman in 1930.[25] In effect, this technique uses transfer learning to avoid catastrophic interference, by making a net's responses to new classes as consistent as possible with existing responses to classes already learned.\n\nKirkpatrick et al. (2017)[26] demonstrated a method to train a single artificial neural network on multiple tasks using a technique called elastic weight consolidation.\n\nPractopoietic theory[27] proposes that biological systems solve the problem of catastrophic interference by storing long-term memories only in a general form, not applicable to a given situation but instead loosely applicable to a class of different situations. In order to adjust the loosely applicable knowledge to the given current situation, the process of anapoiesis is applied. Anapoiesis stands for \"reconstruction of knowledge\"—transforming knowledge from a general form to a specific one. Practopoietic theory is founded in the theorems of cybernetics and is concerned with the question of how cybernetic systems obtain their capabilities to control and act.\n\n"
    }
  ],
  "links": []
}